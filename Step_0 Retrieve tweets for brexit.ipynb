{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf22d0d",
   "metadata": {},
   "source": [
    "### Step_0 Retrieve tweets for brexit\n",
    "In this notebook we retrive from twitter tweets that are relevant to brexit from different time-frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da172a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:30:25.255474Z",
     "start_time": "2022-06-18T08:30:20.604710Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv , datetime, unicodedata, time, tweeterid #dateutil.parserm,\n",
    "import gensim\n",
    "import openpyxl\n",
    "import json\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from inputimeout import inputimeout, TimeoutOccurred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b74142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:30:25.502334Z",
     "start_time": "2022-06-18T08:30:25.484352Z"
    }
   },
   "outputs": [],
   "source": [
    "from Brexit_Package import TwitterCrawler\n",
    "\n",
    "my_token = 'NLP_is_fun' ### Of course one need to provide here a real valid token, unfortunelty we can't publish our token...\n",
    "#my_token = \"AAAAAAAAAAAAAAAAAAAAAPr2WAEAAAAARCTqXt1KnbCbnG4FzY8S8A5zoYg%3DYWyy0y9SQtwDenwjkU8UB662tfhm8yxZl3Ud3T3aEerCMT6B9W\" #roy madpis\n",
    "my_token = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT' #idc\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892e352c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:30:25.706221Z",
     "start_time": "2022-06-18T08:30:25.695224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "limit_amount_of_returned_tweets = 25000\n",
    "max_results = 500\n",
    "dir_name = \"Roy_tweets_brexit\"\n",
    "query = \"(Brexit OR BREXIT OR brexit OR BRexit OR BREX OR brex OR brexi OR Brexi) lang:en -is:retweet -is:reply -is:quote -from:theresa_may -from:NicolaSturgeon -from:MichelBarnier -from:fhollande -from:PhilipHammondUK -from:DavidDavisMP -from:JunckerEU -from:guyverhofstadt -from:EndaKennyTD -from:hilarybennmp -from:MinPres -from:MartinSelmayr -from:Keir_Starmer -from:LiamFox -from:TimmermansEU -from:BorisJohnson -from:Nigel_Farage -from:ManfredWeber -from:davidmcallister -from:matteorenzi -from:SadiqKhan -from:cbicarolyn -from:rupertmurdoch -from:WeyandSabine -from:MLP_officiel -from:FrancesOGrady -from:giannipittella -from:MalmstromEU -from:TheGinaMiller -from:CER_Grant -from:AMCarwyn -from:AlunCairns -from:DominicRaab  -from:michaelgove  -from:theJeremyVine -from:George_Osborne  -from:michaelhsltine_  -from:WilliamJHague -from:ChukaUmunna -from:jeremycorbyn -from:Dyson -from:SteveO_Connell -from:MorrisseyHelena -from:GroovyTimbo\"\n",
    " \n",
    "\n",
    "print(limit_amount_of_returned_tweets*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d9304",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0667c90f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T11:36:30.966871Z",
     "start_time": "2022-06-17T11:36:10.002695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2014 already exist\n",
      "no more tweets from this user\n",
      "Total amount of collected tweets =  4252\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2014-01-01T00:00:00Z\"\n",
    "end_time = \"2014-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2014\"\n",
    "limit_amount_of_returned_tweets = 40000\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = False, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc10f9",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687a30f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:02:02.996615Z",
     "start_time": "2022-06-17T11:56:03.878320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2015 already exist\n",
      "1 Got from twitter 487 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 490 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 494 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25002 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2015-01-01T00:00:00Z\"\n",
    "end_time = \"2015-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2015\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62206657",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fcc5c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:07:09.721477Z",
     "start_time": "2022-06-17T12:02:33.530666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2016 already exist\n",
      "1 Got from twitter 495 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 493 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 498 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25009 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2016-01-01T00:00:00Z\"\n",
    "end_time = \"2016-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2016\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94327ba6",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc6e064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:13:40.392640Z",
     "start_time": "2022-06-17T12:07:32.978863Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2017 already exist\n",
      "1 Got from twitter 489 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 488 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 492 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25004 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2017-01-01T00:00:00Z\"\n",
    "end_time = \"2017-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2017\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f066e1",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b457f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:19:50.996707Z",
     "start_time": "2022-06-17T12:14:05.541970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2018 already exist\n",
      "1 Got from twitter 489 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 493 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 482 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25003 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2018-01-01T00:00:00Z\"\n",
    "end_time = \"2018-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2018\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb160cd3",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4541d87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:26:13.731308Z",
     "start_time": "2022-06-17T12:20:32.425998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2019 already exist\n",
      "1 Got from twitter 484 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 490 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 478 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25007 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2019-01-01T00:00:00Z\"\n",
    "end_time = \"2019-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2019\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb8e4f",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc46d00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:30:55.372093Z",
     "start_time": "2022-06-17T12:26:38.733891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2020 already exist\n",
      "1 Got from twitter 487 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 482 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 487 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25001 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2020-01-01T00:00:00Z\"\n",
    "end_time = \"2020-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2020\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587630dc",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cf2c94c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:36:43.505644Z",
     "start_time": "2022-06-17T12:31:23.882169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2021 already exist\n",
      "1 Got from twitter 497 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 494 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 490 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25008 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2021-01-01T00:00:00Z\"\n",
    "end_time = \"2021-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2021\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7a5d2",
   "metadata": {},
   "source": [
    "## Retrieve tweets given tweet-id\n",
    "We have 4.5M tweet-ids with classification for positive/negative/neutral tweets,\n",
    "We wish to read them from twitter using the Brexit Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6e6b1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:09:59.056203Z",
     "start_time": "2022-06-18T07:07:13.181003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c52d34054c4067b06d08e36efc13c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 6: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1424/289699957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msentiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation_of_txt_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mtweet_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1255.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 6: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "### Reading all the tweet-ids + the sentiment and save those in a table\n",
    "location_of_txt_files = \"nlp-project/data_folder/tweets_4_5M/Brexit_tweets_stance\"\n",
    "\n",
    "#tqdm_notebook(filtered_table.clean_text):\n",
    "df_all = pd.DataFrame({\"tweet_id\":[0], \"sentimnt\":[0]})\n",
    "\n",
    "for i, file in tqdm_notebook(enumerate(os.listdir(location_of_txt_files))):\n",
    "    tweet_ids= []\n",
    "    sentiments = []\n",
    "    try:\n",
    "        with open(os.path.join(location_of_txt_files, file)) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                tweet_id, sentiment = line.split()\n",
    "                tweet_ids.append(tweet_id)\n",
    "                sentiments.append(sentiment)\n",
    "            df = pd.DataFrame({\"tweet_id\":tweet_ids, \"sentimnt\":sentiments})\n",
    "            df_all = pd.concat([df_all, df])\n",
    "    except:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4dddd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:29:08.085440Z",
     "start_time": "2022-06-18T08:29:08.070471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentimnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>730759428430008320</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>730759413938655232</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>730759400445644801</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>730759393965412352</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>730759392925261824</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>730759383253192705</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>730759377548918784</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>730759375174897664</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>730759372603789313</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>730759366530478080</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id  sentimnt\n",
       "0           0  730759428430008320  Positive\n",
       "1           1  730759413938655232  Negative\n",
       "2           2  730759400445644801  Positive\n",
       "3           3  730759393965412352   Neutral\n",
       "4           4  730759392925261824  Positive\n",
       "5           5  730759383253192705   Neutral\n",
       "6           6  730759377548918784  Negative\n",
       "7           7  730759375174897664   Neutral\n",
       "8           8  730759372603789313  Negative\n",
       "9           9  730759366530478080   Neutral"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35b6a901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:15:14.031592Z",
     "start_time": "2022-06-18T07:15:12.892246Z"
    }
   },
   "outputs": [],
   "source": [
    "### save all the data in a file\n",
    "df_all = df_all[df_all[\"tweet_id\"]!=0]\n",
    "print(\"number of tweets:\", df_all.shape[0])\n",
    "df_all = df_all.drop_duplicates(subset=[\"tweet_id\"])\n",
    "print(\"number of tweets after removing duplicate ids:\", df_all.shape[0])\n",
    "print(\"Saving the table in a csv file\")\n",
    "df_all.to_csv(os.path.join(\"data_folder/tweets_4_5M\", \"all_tweetids_and_semtiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50bda12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:27:03.213988Z",
     "start_time": "2022-06-18T09:26:59.540049Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_all= pd.read_csv(\"data_folder/tweets_4_5M/all_tweetids_and_semtiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e404be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:17:58.250256Z",
     "start_time": "2022-06-18T07:17:58.243263Z"
    }
   },
   "source": [
    "# Reading the tweets by tweet id and add the sentiment that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220c8675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:30:50.736302Z",
     "start_time": "2022-06-18T08:30:50.728309Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_ids = [\"730759400445644801\", \"730759393965412352\", \"730759392925261824\", \"730759377548918784\", \"730759372603789313\", \"730759366530478080\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327bc91",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-18T08:30:56.611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ Already sent 0 requests\n",
      "~~~~~ Already proccesed 0 tweets\n",
      "~~~~~ Currently proccessing 2 tweets\n",
      "Failed to connect 6 times, going to sleep for 15 minutes..\n"
     ]
    }
   ],
   "source": [
    "json_response_list = my_twitter_crawler.get_tweets_by_tweet_ids(tweet_ids,\n",
    "     json_tweets_output_folder = \"tweet_ids_json\",\n",
    "     tweets_per_api_request = 2,\n",
    "     api_error_sleep_secs = 15*60,\n",
    "     verbose = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cbf9b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-18T08:29:53.863Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "json_response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d68568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:15:10.469466Z",
     "start_time": "2022-06-18T08:15:10.455476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'source': 'Twitter for Android',\n",
       "   'id': '730759400445644801',\n",
       "   'conversation_id': '730759400445644801',\n",
       "   'lang': 'en',\n",
       "   'referenced_tweets': [{'type': 'retweeted', 'id': '730752213220524032'}],\n",
       "   'created_at': '2016-05-12T13:59:52.000Z',\n",
       "   'public_metrics': {'retweet_count': 2,\n",
       "    'reply_count': 0,\n",
       "    'like_count': 0,\n",
       "    'quote_count': 0},\n",
       "   'reply_settings': 'everyone',\n",
       "   'text': 'RT @raymach1: @David_Cameron according to you everything in the World will suffer if we #LeaveEU , very desperate argument #callmedave',\n",
       "   'author_id': '68137754'}],\n",
       " 'includes': {'users': [{'created_at': '2009-08-23T12:53:57.000Z',\n",
       "    'public_metrics': {'followers_count': 3485,\n",
       "     'following_count': 4968,\n",
       "     'tweet_count': 174565,\n",
       "     'listed_count': 57},\n",
       "    'verified': False,\n",
       "    'name': \"Andy RV Harvey #🇬🇧 Boris's Britain Is Great 🇬🇧\",\n",
       "    'username': 'andyrv1960',\n",
       "    'description': 'Bristol Lad & Proud Gashead UTG\\nNorthern Soul & Scooter Scene 🎶🛵\\nGETTR = @AndyRvHarvey\\n#BackBoris 🇬🇧\\n#GodSaveTheQueen 🇬🇧🏴\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f🇬🇧',\n",
       "    'id': '68137754'}]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response = json_response_list[0]\n",
    "\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d18be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:15:17.456631Z",
     "start_time": "2022-06-18T08:15:17.379690Z"
    }
   },
   "outputs": [],
   "source": [
    "a = pd.json_normalize(json_response[\"data\"])\n",
    "b = pd.json_normalize(json_response[\"includes\"], [\"users\"]).add_prefix(\"users.\")\n",
    "\n",
    "a.conversation_id = a.conversation_id.astype(\"string\")\n",
    "a.id = a.id.astype(\"string\")\n",
    "a[\"id_new\"] = \"id: \" + a[\"id\"].astype(\"string\")\n",
    "a[\"conv_id_new\"] = \"conv_id: \" + a[\"conversation_id\"].astype(\"string\")\n",
    "a[\"author_id_new\"] = \"author_id: \" + a[\"author_id\"].astype(\"string\")\n",
    "\n",
    "#c = pd.json_normalize(json_response[\"includes\"][\"places\"]).add_prefix(\"places.\")\n",
    "df_tweets_i = pd.merge(a, b, left_on=\"author_id\", right_on=\"users.id\")\n",
    "list_of_cols_to_add = ['author_id', \"author_id_new\", 'conversation_id', \"conv_id_new\",\n",
    "                        \"id\", \"id_new\",'created_at','entities.mentions',\n",
    "            'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "        'public_metrics.reply_count', 'public_metrics.retweet_count','referenced_tweets', 'text',\n",
    "            'users.created_at', 'users.description','users.id', 'users.name',\n",
    "        'users.public_metrics.followers_count', 'users.public_metrics.following_count',\n",
    "        'users.public_metrics.listed_count', 'users.public_metrics.tweet_count',\n",
    "        'users.username', 'users.verified']\n",
    "\n",
    "list_cols_to_drop = [x for x in df_tweets_i.columns if x not in list_of_cols_to_add]\n",
    "\n",
    "##droping labels (columns) we don't need\n",
    "df_tweets_i = df_tweets_i.drop(labels=list_cols_to_drop, axis = 1, errors = \"ignore\")\n",
    "\n",
    "for col in list_of_cols_to_add:\n",
    "    if col not in df_tweets_i.columns:\n",
    "        df_tweets_i[col] = \"NA\"\n",
    "\n",
    "#sort columns by alphabetic order\n",
    "col_list_df_tweets_i = df_tweets_i.columns.tolist()\n",
    "col_list_df_tweets_i.sort()\n",
    "df_tweets_i = df_tweets_i.reindex(columns=col_list_df_tweets_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "213ae7cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:15:23.709913Z",
     "start_time": "2022-06-18T08:15:23.648947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_id_new</th>\n",
       "      <th>conv_id_new</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities.mentions</th>\n",
       "      <th>id</th>\n",
       "      <th>id_new</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>users.created_at</th>\n",
       "      <th>users.description</th>\n",
       "      <th>users.id</th>\n",
       "      <th>users.name</th>\n",
       "      <th>users.public_metrics.followers_count</th>\n",
       "      <th>users.public_metrics.following_count</th>\n",
       "      <th>users.public_metrics.listed_count</th>\n",
       "      <th>users.public_metrics.tweet_count</th>\n",
       "      <th>users.username</th>\n",
       "      <th>users.verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68137754</td>\n",
       "      <td>author_id: 68137754</td>\n",
       "      <td>conv_id: 730759400445644801</td>\n",
       "      <td>730759400445644801</td>\n",
       "      <td>2016-05-12T13:59:52.000Z</td>\n",
       "      <td>NA</td>\n",
       "      <td>730759400445644801</td>\n",
       "      <td>id: 730759400445644801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009-08-23T12:53:57.000Z</td>\n",
       "      <td>Bristol Lad &amp; Proud Gashead UTG\\nNorthern Soul...</td>\n",
       "      <td>68137754</td>\n",
       "      <td>Andy RV Harvey #🇬🇧 Boris's Britain Is Great 🇬🇧</td>\n",
       "      <td>3485</td>\n",
       "      <td>4968</td>\n",
       "      <td>57</td>\n",
       "      <td>174565</td>\n",
       "      <td>andyrv1960</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id        author_id_new                  conv_id_new  \\\n",
       "0  68137754  author_id: 68137754  conv_id: 730759400445644801   \n",
       "\n",
       "      conversation_id                created_at entities.mentions  \\\n",
       "0  730759400445644801  2016-05-12T13:59:52.000Z                NA   \n",
       "\n",
       "                   id                  id_new  public_metrics.like_count  \\\n",
       "0  730759400445644801  id: 730759400445644801                          0   \n",
       "\n",
       "   public_metrics.quote_count  ...          users.created_at  \\\n",
       "0                           0  ...  2009-08-23T12:53:57.000Z   \n",
       "\n",
       "                                   users.description  users.id  \\\n",
       "0  Bristol Lad & Proud Gashead UTG\\nNorthern Soul...  68137754   \n",
       "\n",
       "                                       users.name  \\\n",
       "0  Andy RV Harvey #🇬🇧 Boris's Britain Is Great 🇬🇧   \n",
       "\n",
       "  users.public_metrics.followers_count users.public_metrics.following_count  \\\n",
       "0                                 3485                                 4968   \n",
       "\n",
       "  users.public_metrics.listed_count users.public_metrics.tweet_count  \\\n",
       "0                                57                           174565   \n",
       "\n",
       "   users.username  users.verified  \n",
       "0      andyrv1960           False  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2a559b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:45:35.745965Z",
     "start_time": "2022-06-18T07:45:35.738990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if len(tweet_ids)>1:\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4f4b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:59:58.418307Z",
     "start_time": "2022-06-18T07:59:58.409312Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_ids = list(range(1000))\n",
    "n = 100\n",
    "batches=[tweet_ids[i:i + n] for i in range(0, len(tweet_ids), n)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac80519c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:36:14.574716Z",
     "start_time": "2022-06-18T09:36:14.565741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ Already sent 1 requests\n",
      "~~~~~ Already proccesed 10 tweets\n",
      "~~~~~ Currently proccessing 2 tweets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "count_proccesed_tweets = 10\n",
    "\n",
    "count_requests_sent = 1\n",
    "tweets_per_api_request = 2\n",
    "\n",
    "\n",
    "print(f'~~~~~ Already sent {count_requests_sent} requests')\n",
    "print(f'~~~~~ Already proccesed {count_proccesed_tweets} tweets')\n",
    "print(f'~~~~~ Currently proccessing {tweets_per_api_request} tweets')\n",
    "\n",
    "\n",
    "## FIXME - last batch size cant be more than what's left\n",
    "if len(tweet_ids)>1:\n",
    "    tweet_ids_batch = tweet_ids[count_proccesed_tweets:(count_proccesed_tweets+ tweets_per_api_request )// (len(tweet_ids)-1)]\n",
    "else:\n",
    "    tweet_ids_batch = tweet_ids\n",
    "\n",
    "    #tweet_ids_batch = tweet_ids[count_proccesed_tweets:(count_proccesed_tweets+ tweets_per_api_request )// (len(tweet_ids)-1)]\n",
    "    #search_url, query_params = self.create_url_tweet_ids(\"https://api.twitter.com/2/tweets/?ids=X\" , tweet_ids_batch)\n",
    "    # headers = create_headers(os.environ['TOKEN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae2f536b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:34:45.350247Z",
     "start_time": "2022-06-18T09:34:45.341254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[730759365142167552,\n",
       " 730759363175034880,\n",
       " 730759346766876672,\n",
       " 730759344879505409,\n",
       " 730759334796349440]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cb19a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:36:00.141216Z",
     "start_time": "2022-06-18T09:36:00.134237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(tweet_ids)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "939ced1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:36:20.170612Z",
     "start_time": "2022-06-18T09:36:20.162641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_proccesed_tweets+ tweets_per_api_request // (len(tweet_ids)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17a44e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:36:36.451380Z",
     "start_time": "2022-06-18T09:36:36.441391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[730759346766876672, 730759344879505409, 730759334796349440]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_ids[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7644fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "    def get_tweets_by_tweet_ids(self , tweet_ids, #TypeError: 'type' object is not subscriptable\n",
    "     json_tweets_output_folder : str,\n",
    "     tweets_per_api_request : int = 100,\n",
    "     api_error_sleep_secs : int = 15*60,\n",
    "     verbose = False ):\n",
    "        \n",
    "        json_response_list = []\n",
    "        os.makedirs(json_tweets_output_folder, exist_ok = True)\n",
    "\n",
    "        count_proccesed_tweets_file = os.path.join(json_tweets_output_folder, 'count_proccesed_tweets.txt')\n",
    "        count_requests_sent_file = os.path.join(json_tweets_output_folder,'count_requests_sent.txt')\n",
    "        json_output_file_basename = os.path.join(json_tweets_output_folder,'twitter_data_')\n",
    "\n",
    "        if not os.path.exists(count_requests_sent_file):\n",
    "            with open(count_requests_sent_file,'w') as f:\n",
    "                f.write('%d' % 0)\n",
    "\n",
    "        if not os.path.exists(count_proccesed_tweets_file):\n",
    "            with open(count_proccesed_tweets_file,'w') as f:\n",
    "                f.write('%d' % 0)\n",
    "\n",
    "        count_proccesed_tweets = int( open(count_proccesed_tweets_file,'r').read())\n",
    "\n",
    "        count_requests_sent = int( open(count_requests_sent_file,'r').read())\n",
    "\n",
    "        while(count_proccesed_tweets < len(tweet_ids)):\n",
    "\n",
    "            if verbose:\n",
    "                print(f'~~~~~ Already sent {count_requests_sent} requests')\n",
    "                print(f'~~~~~ Already proccesed {count_proccesed_tweets} tweets')\n",
    "                print(f'~~~~~ Currently proccessing {tweets_per_api_request} tweets')\n",
    "\n",
    "\n",
    "            ## FIXME - last batch size cant be more than what's left\n",
    "            if len(tweet_ids)>1:\n",
    "                tweet_ids_batch = tweet_ids[count_proccesed_tweets:(count_proccesed_tweets+ tweets_per_api_request )// (len(tweet_ids)-1)]\n",
    "            else:\n",
    "                tweet_ids_batch = tweet_ids\n",
    "            \n",
    "            #tweet_ids_batch = tweet_ids[count_proccesed_tweets:(count_proccesed_tweets+ tweets_per_api_request )// (len(tweet_ids)-1)]\n",
    "            search_url, query_params = self.create_url_tweet_ids(\"https://api.twitter.com/2/tweets/?ids=X\" , tweet_ids_batch)\n",
    "            # headers = create_headers(os.environ['TOKEN'])\n",
    "\n",
    "            try:\n",
    "                json_response = self.__connect_to_endpoint( url = search_url, params= query_params)\n",
    "                json_response_list.append(json_response)\n",
    "            except Exception as e:\n",
    "                print(f'Twitter API error: {e} \\n Sleeping 15 min from {datetime.datetime.now()}')\n",
    "                time.sleep(api_error_sleep_secs)\n",
    "\n",
    "                \n",
    "            count_proccesed_tweets += tweets_per_api_request\n",
    "            count_requests_sent += 1\n",
    "\n",
    "\n",
    "            with open(f'{json_output_file_basename}{count_requests_sent}.json', \"w\") as twitter_data_file:\n",
    "                json.dump(json_response, twitter_data_file, indent=4, sort_keys=True)\n",
    "\n",
    "            with open(count_proccesed_tweets_file, \"w\") as f:\n",
    "                f.write('%d' % count_proccesed_tweets)\n",
    "\n",
    "            with open(count_requests_sent_file, \"w\") as f:\n",
    "                f.write('%d' % count_requests_sent)\n",
    "                \n",
    "        return(json_response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcb32b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:16:40.699963Z",
     "start_time": "2022-06-18T09:16:40.665984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_ids = list(range(10))\n",
    "n = 100\n",
    "batches=[tweet_ids[i:i + n] for i in range(0, len(tweet_ids), n)]\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4874c0fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:17:10.961406Z",
     "start_time": "2022-06-18T09:17:10.720550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4248d311ac46cf92b6f34a8332bb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "for batch in tqdm_notebook(batches):\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### try using the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e17397e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:13:45.402258Z",
     "start_time": "2022-06-18T10:13:41.545657Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv , datetime, unicodedata, time, tweeterid #dateutil.parserm,\n",
    "import gensim\n",
    "import openpyxl\n",
    "import json\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from inputimeout import inputimeout, TimeoutOccurred\n",
    "\n",
    "from Brexit_Package import TwitterCrawler\n",
    "\n",
    "my_token = 'NLP_is_fun' ### Of course one need to provide here a real valid token, unfortunelty we can't publish our token...\n",
    "#my_token = \"AAAAAAAAAAAAAAAAAAAAAPr2WAEAAAAARCTqXt1KnbCbnG4FzY8S8A5zoYg%3DYWyy0y9SQtwDenwjkU8UB662tfhm8yxZl3Ud3T3aEerCMT6B9W\" #roy madpis\n",
    "my_token = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT' #idc\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444b50e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:13:51.108786Z",
     "start_time": "2022-06-18T10:13:47.840655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets we have sentiment labels on them: 5499483\n"
     ]
    }
   ],
   "source": [
    "### reading the table with all the tweets ids we know their sentiment, there are about 5M tweets\n",
    "df_all= pd.read_csv(\"data_folder/tweets_4_5M/all_tweetids_and_semtiment.csv\")\n",
    "print(\"Number of tweets we have sentiment labels on them:\", df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90c24ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:13:52.072234Z",
     "start_time": "2022-06-18T10:13:52.048250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "### we will not export all those 5M tweets, we will retrive 100K of them\n",
    "total_amount_of_tweets_we_want = 10000\n",
    "tweet_ids_n = list(df_all.tweet_id[0:total_amount_of_tweets_we_want])\n",
    "print(len(tweet_ids_n))\n",
    "\n",
    "### turn the tweet-ids from numbers to strings\n",
    "tweet_ids = []\n",
    "for i in tweet_ids_n:\n",
    "    tweet_ids.append(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44775aae",
   "metadata": {},
   "source": [
    "### check which tweet ids were already evaluated\n",
    "In this way we won't read the same tweet-id twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92feb1a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:13:53.921772Z",
     "start_time": "2022-06-18T10:13:53.893789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir tweets_by_tweet_ids already exist\n",
      "There are 9000 tweet ids to search; see here the first 20:\n",
      "['730831165523951616', '730794312917778432', '730795577521393665', '730795247119290368', '730814977309868032', '730788952517529601', '730813060198211584', '730825230231441408', '730837460570284041', '730823431906861057', '730804618771464192', '730831894208757760', '730842728184074241', '730818447752478720', '730820491829772288', '730835582117646336', '730820128447864832', '730818630833692672', '730814405831733250', '730790674019147776']\n",
      "\n",
      "There are 1000 Tweet ids we already have data on --> here are the first 20:\n",
      "['730759365142167552', '730759363175034880', '730759334796349440', '730759346766876672', '730759344879505409', '730759317884915712', '730759310754611201', '730759317075439617', '730759295478960129', '730759305792749568', '730759304253452289', '730759296955367424', '730759278630424577', '730759264386617344', '730759228223279104', '730759251883347968', '730759256530661377', '730759212268052480', '730759244979396608', '730759212163305472']\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##################################\n",
    "all_tweet_ids = tweet_ids\n",
    "dir_name = \"tweets_by_tweet_ids\"\n",
    "csv_table_name = \"brexit_tweet_ids\"\n",
    "\n",
    "##############################################################################################3\n",
    "#### we don't want to get data on tweet-ids that we already searched for their tweets!\n",
    "# thus we first need to see which of the tweet-ids that are inserted in the list are **not** in the log-file we already have:\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os.path\n",
    "try:\n",
    "    os.mkdir(dir_name)\n",
    "    print(\"creating directory\", dir_name, \"to insert all the tables of all the tweet-idss\")\n",
    "except:\n",
    "    print(\"The dir\", dir_name ,\"already exist\")\n",
    "\n",
    "location_of_log_tweetids_txt_file = os.path.join(dir_name, \"log_tweets\", csv_table_name, \"evaluated_tweet_ids.txt\")\n",
    "\n",
    "#tqdm_notebook(filtered_table.clean_text):\n",
    "evaluated_tweet_ids = []\n",
    "try:\n",
    "    with open(location_of_log_tweetids_txt_file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tweet_id = line.split()\n",
    "            evaluated_tweet_ids.append(tweet_id[0])        \n",
    "except:\n",
    "    print(\"file not existed\")\n",
    "\n",
    "tweet_ids_we_have_data = evaluated_tweet_ids\n",
    "tweet_ids_to_search = list(set(all_tweet_ids) - set(tweet_ids_we_have_data))\n",
    "    \n",
    "if len(tweet_ids_to_search) > 20:\n",
    "    print(\"There are\", len(tweet_ids_to_search), \"tweet ids to search; see here the first 20:\")\n",
    "    print(tweet_ids_to_search[0:20])\n",
    "else:\n",
    "    print(\"Tweet ids to search:\",tweet_ids_to_search)\n",
    "    print(\"-----------------------------------------------\")\n",
    "print()\n",
    "if len(tweet_ids_we_have_data) > 20:\n",
    "    print(\"There are\", len(tweet_ids_we_have_data), \"Tweet ids we already have data on --> here are the first 20:\")\n",
    "    print(tweet_ids_we_have_data[0:20])\n",
    "else : print(\"Tweet-ids we already have data on:\",list(set(all_tweet_ids).intersection(tweet_ids_we_have_data)))\n",
    "\n",
    "#print(\"Tweet-ids to search:\",tweet_ids_to_search)\n",
    "print(\"-----------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00bd76",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-18T10:14:03.892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir tweets_by_tweet_ids already exist\n",
      "The dir tweets_by_tweet_ids\\log_tweets already exist\n",
      "The dir tweets_by_tweet_ids\\log_tweets\\brexit_tweet_ids already exist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f069444d784373a128e65d95b14b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_response_list, num_of_returned_tweets, problematic_batches = my_twitter_crawler.return_tweets_given_tweet_ids_new(\n",
    "    tweet_ids=tweet_ids_to_search,\n",
    "    number_of_tweets_in_batch = 100,\n",
    "    verbose_10 = True, dir_name = dir_name,\n",
    "    csv_table_name = csv_table_name,\n",
    "    api_error_sleep_secs = 15*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89379237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:50:12.301521Z",
     "start_time": "2022-06-18T09:50:12.293530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brexit_tweet_ids'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d5f1fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:51:10.324061Z",
     "start_time": "2022-06-18T09:51:10.307074Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e846aaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:11:21.837641Z",
     "start_time": "2022-06-18T10:11:13.091389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets we read from twitter: 522\n",
      "Number of tweets with sentiment: 522\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_id_new</th>\n",
       "      <th>conv_id_new</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities.mentions</th>\n",
       "      <th>id</th>\n",
       "      <th>id_new</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>users.id</th>\n",
       "      <th>users.name</th>\n",
       "      <th>users.public_metrics.followers_count</th>\n",
       "      <th>users.public_metrics.following_count</th>\n",
       "      <th>users.public_metrics.listed_count</th>\n",
       "      <th>users.public_metrics.tweet_count</th>\n",
       "      <th>users.username</th>\n",
       "      <th>users.verified</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentimnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137099503</td>\n",
       "      <td>author_id: 137099503</td>\n",
       "      <td>conv_id: 730759365142167552</td>\n",
       "      <td>730759365142167552</td>\n",
       "      <td>2016-05-12T13:59:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759365142167552</td>\n",
       "      <td>id: 730759365142167552</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137099503</td>\n",
       "      <td>Fony Blair</td>\n",
       "      <td>463</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>22398</td>\n",
       "      <td>FonyBlair</td>\n",
       "      <td>False</td>\n",
       "      <td>730759365142167552</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367718336</td>\n",
       "      <td>author_id: 367718336</td>\n",
       "      <td>conv_id: 730759317884915712</td>\n",
       "      <td>730759317884915712</td>\n",
       "      <td>2016-05-12T13:59:32.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759317884915712</td>\n",
       "      <td>id: 730759317884915712</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>367718336</td>\n",
       "      <td>Janice Morphet</td>\n",
       "      <td>3623</td>\n",
       "      <td>5000</td>\n",
       "      <td>433</td>\n",
       "      <td>301390</td>\n",
       "      <td>janicemorphet</td>\n",
       "      <td>False</td>\n",
       "      <td>730759317884915712</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1299769218</td>\n",
       "      <td>author_id: 1299769218</td>\n",
       "      <td>conv_id: 730759295478960129</td>\n",
       "      <td>730759295478960129</td>\n",
       "      <td>2016-05-12T13:59:27.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759295478960129</td>\n",
       "      <td>id: 730759295478960129</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1299769218</td>\n",
       "      <td>Kay Burley</td>\n",
       "      <td>558874</td>\n",
       "      <td>359</td>\n",
       "      <td>1630</td>\n",
       "      <td>81943</td>\n",
       "      <td>KayBurley</td>\n",
       "      <td>True</td>\n",
       "      <td>730759295478960129</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2772884435</td>\n",
       "      <td>author_id: 2772884435</td>\n",
       "      <td>conv_id: 730759278630424577</td>\n",
       "      <td>730759278630424577</td>\n",
       "      <td>2016-05-12T13:59:23.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759278630424577</td>\n",
       "      <td>id: 730759278630424577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2772884435</td>\n",
       "      <td>Risethefyrd</td>\n",
       "      <td>1206</td>\n",
       "      <td>1415</td>\n",
       "      <td>25</td>\n",
       "      <td>31524</td>\n",
       "      <td>ernieharding59</td>\n",
       "      <td>False</td>\n",
       "      <td>730759278630424577</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2867356894</td>\n",
       "      <td>author_id: 2867356894</td>\n",
       "      <td>conv_id: 730759264386617344</td>\n",
       "      <td>730759264386617344</td>\n",
       "      <td>2016-05-12T13:59:20.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759264386617344</td>\n",
       "      <td>id: 730759264386617344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2867356894</td>\n",
       "      <td>Dr C L Spillard</td>\n",
       "      <td>2922</td>\n",
       "      <td>2474</td>\n",
       "      <td>122</td>\n",
       "      <td>64418</td>\n",
       "      <td>CandiSpillard</td>\n",
       "      <td>False</td>\n",
       "      <td>730759264386617344</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id          author_id_new                  conv_id_new  \\\n",
       "0   137099503   author_id: 137099503  conv_id: 730759365142167552   \n",
       "1   367718336   author_id: 367718336  conv_id: 730759317884915712   \n",
       "2  1299769218  author_id: 1299769218  conv_id: 730759295478960129   \n",
       "3  2772884435  author_id: 2772884435  conv_id: 730759278630424577   \n",
       "4  2867356894  author_id: 2867356894  conv_id: 730759264386617344   \n",
       "\n",
       "      conversation_id                created_at  entities.mentions  \\\n",
       "0  730759365142167552  2016-05-12T13:59:44.000Z                NaN   \n",
       "1  730759317884915712  2016-05-12T13:59:32.000Z                NaN   \n",
       "2  730759295478960129  2016-05-12T13:59:27.000Z                NaN   \n",
       "3  730759278630424577  2016-05-12T13:59:23.000Z                NaN   \n",
       "4  730759264386617344  2016-05-12T13:59:20.000Z                NaN   \n",
       "\n",
       "                   id                  id_new  public_metrics.like_count  \\\n",
       "0  730759365142167552  id: 730759365142167552                          2   \n",
       "1  730759317884915712  id: 730759317884915712                          0   \n",
       "2  730759295478960129  id: 730759295478960129                          4   \n",
       "3  730759278630424577  id: 730759278630424577                          0   \n",
       "4  730759264386617344  id: 730759264386617344                          0   \n",
       "\n",
       "   public_metrics.quote_count  ...    users.id       users.name  \\\n",
       "0                           0  ...   137099503       Fony Blair   \n",
       "1                           0  ...   367718336   Janice Morphet   \n",
       "2                           0  ...  1299769218       Kay Burley   \n",
       "3                           0  ...  2772884435      Risethefyrd   \n",
       "4                           0  ...  2867356894  Dr C L Spillard   \n",
       "\n",
       "  users.public_metrics.followers_count users.public_metrics.following_count  \\\n",
       "0                                  463                                   71   \n",
       "1                                 3623                                 5000   \n",
       "2                               558874                                  359   \n",
       "3                                 1206                                 1415   \n",
       "4                                 2922                                 2474   \n",
       "\n",
       "  users.public_metrics.listed_count users.public_metrics.tweet_count  \\\n",
       "0                                18                            22398   \n",
       "1                               433                           301390   \n",
       "2                              1630                            81943   \n",
       "3                                25                            31524   \n",
       "4                               122                            64418   \n",
       "\n",
       "   users.username users.verified            tweet_id  sentimnt  \n",
       "0       FonyBlair          False  730759365142167552   Neutral  \n",
       "1   janicemorphet          False  730759317884915712   Neutral  \n",
       "2       KayBurley           True  730759295478960129   Neutral  \n",
       "3  ernieharding59          False  730759278630424577  Positive  \n",
       "4   CandiSpillard          False  730759264386617344  Negative  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read dataframe with all the tweet-ids and the data on them - that we exported from twitter\n",
    "tweets_no_sentiment = pd.read_csv(os.path.join(dir_name,csv_table_name + \".csv\"))\n",
    "\n",
    "### cleaning the data and remove duplicates\n",
    "tweets_no_sentiment = tweets_no_sentiment.drop(labels = [\"Unnamed: 0\"], axis = 1)\n",
    "tweets_no_sentiment = tweets_no_sentiment.drop_duplicates(subset=[\"id\", \"created_at\"])\n",
    "print(\"Number of tweets we read from twitter:\", tweets_no_sentiment.shape[0])\n",
    "\n",
    "### read dataframe with all the tweet-ids and the sentiment\n",
    "df_all= pd.read_csv(\"data_folder/tweets_4_5M/all_tweetids_and_semtiment.csv\")\n",
    "df_all = df_all.drop(labels = [\"Unnamed: 0\"], axis = 1)\n",
    "\n",
    "### Merging the two tables so we will have one table with the tweets data + sentiment labels\n",
    "tweets_with_sentiment = pd.merge(left = tweets_no_sentiment, right = df_all, how = \"inner\", left_on='id', right_on='tweet_id')\n",
    "print(\"Number of tweets with sentiment:\", tweets_with_sentiment.shape[0])\n",
    "\n",
    "tweets_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6f43113",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:53:49.912828Z",
     "start_time": "2022-06-18T09:53:49.894837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentimnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>730759428430008320</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>730759413938655232</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>730759400445644801</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>730759393965412352</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>730759392925261824</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499478</th>\n",
       "      <td>3836</td>\n",
       "      <td>746538227079667712</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499479</th>\n",
       "      <td>3837</td>\n",
       "      <td>746538225695490048</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499480</th>\n",
       "      <td>3838</td>\n",
       "      <td>746538225020379136</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499481</th>\n",
       "      <td>3839</td>\n",
       "      <td>746538224554639360</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499482</th>\n",
       "      <td>3840</td>\n",
       "      <td>746538203457327105</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5499483 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0            tweet_id  sentimnt\n",
       "0                 0  730759428430008320  Positive\n",
       "1                 1  730759413938655232  Negative\n",
       "2                 2  730759400445644801  Positive\n",
       "3                 3  730759393965412352   Neutral\n",
       "4                 4  730759392925261824  Positive\n",
       "...             ...                 ...       ...\n",
       "5499478        3836  746538227079667712   Neutral\n",
       "5499479        3837  746538225695490048   Neutral\n",
       "5499480        3838  746538225020379136   Neutral\n",
       "5499481        3839  746538224554639360  Positive\n",
       "5499482        3840  746538203457327105  Negative\n",
       "\n",
       "[5499483 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### return tweets given query\n",
    "\n",
    "\n",
    "    def return_tweets_given_tweet_ids_new(self, tweet_ids=[], number_of_tweets_in_batch = 100,\n",
    "                                           verbose_10 = False, dir_name = \"tweets_by_tweet_ids\",\n",
    "                                  csv_table_name = \"brexit_tweet_ids\", api_error_sleep_secs = 60):\n",
    "        \"\"\" ## Return Tweets of given query\n",
    "\n",
    "This function enalbes getting all the tweets that follow the given query.\\n\n",
    "\n",
    "+ link to twitter-developer page regarding this capability:\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "\n",
    "+ link to twitter-developer page regarding building a query:\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "### App rate limit:\n",
    "        - App rate limit (Application-only): 900 requests per 15-minute window shared among all users of your app\n",
    "        - User rate limit (User context): 900 requests per 15-minute window per each authenticated user\n",
    "        \n",
    "Parameters\n",
    "- query : str --> You can see the full documentation on how to build a query (what to pass in the `query` argument) in the following link:\\n\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/counts/integrate/build-a-query\n",
    "        \n",
    "- dir_name: by defualy \"tweets_by_tweet_ids\" --> the dir name to put all the data from the function\n",
    "\n",
    "- start_date = The start date is the date that from it the function will look and rerieve tweets.\\n\n",
    "Note that the format of the start time should be: \"2015-12-7T00:00:00Z\" (this is the default)\n",
    "\n",
    "- end_date = The end date is the date that till it the function will look and rerieve tweets.\\n\n",
    "Note that the format of the end time should be: \"2021-12-26T00:00:00Z\" (this is the default)\n",
    "\n",
    "- max_results = The max number of tweets to retrieve in a given call. Must be an integer between 10 to 500.\n",
    "- evaluate_last_token - by default = False--> If you have already retrive dataand you wish to continue retriving from the place you stopped, pass \"True\" to this arguemnt.\n",
    "\n",
    "- limit_amount_of_returned_tweets - by default = 10000000 --> Lets say a given account had 5000 tweets in the given timeframe (from the given start to the given end date), and you wish to get only the first 2000, then pass to this arguemnt 2000. It will automatically stop the function when it reaches the `limit_amount_of_returned_tweets` you provided\n",
    "\n",
    "- verbose_10 - by default = False --> If True then the function will print certain things throught the run of the function.    \n",
    "\n",
    "- csv_table_name - the csv name that you wish to give to the table that the function generates. Note that this name will also be the name of the log dir that will contain the tokens and the tweets-json file. This is important as if you wish to continue searching for more tweets of the same query, you need to provide the same csv table name and of course write \"True\" in the argument: evaluate_last_token\n",
    "        \"\"\"\n",
    "        search_url = \"https://api.twitter.com/2/tweets/?ids=X\" #endpoint use to collect data from\n",
    "        \n",
    "        #creating a directory that will contain the csv file + the log directory of the Key Opinion Leader\n",
    "        import os.path\n",
    "        try:\n",
    "            os.mkdir(dir_name)\n",
    "            print(\"creating directory\", dir_name, \"to insert the tables with the tweets for the given tweet-ids\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_name ,\"already exist\")\n",
    "        \n",
    "    ##### Creating the log directory (that will contain 3 files: The token file, the retriving tweets streem and the full json file)\n",
    "        dir_log_name = os.path.join(dir_name, \"log_tweets\")\n",
    "\n",
    "        # Try creating the directory for the logs, if the path exist don't create a new one. Print in either case What happened\n",
    "        try:\n",
    "            os.mkdir(dir_log_name)\n",
    "            print(\"creating directory\", dir_log_name, \"to insert all the logs of the tweets\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_log_name ,\"already exist\")\n",
    "        ######################## creating a path for the specific user that we wish to retirve his tweets\n",
    "        path_for_log_dir_of_certain_query = os.path.join(dir_log_name, csv_table_name)\n",
    "        # Try creating the directory for the specific user, if the path exist don't create a new one. Print in either case What happened\n",
    "        try:\n",
    "            os.mkdir(path_for_log_dir_of_certain_query)\n",
    "            print(\"creating directory\", path_for_log_dir_of_certain_query,\"in the dir\",dir_log_name, \"to insert all the logs of the QUERY\", csv_table_name)\n",
    "        except:\n",
    "            print(\"The dir\", path_for_log_dir_of_certain_query ,\"already exist\")\n",
    "        ### creating the log-text-file of the \"retriving_tweets_streem\" This log file will contain a time-stamp\n",
    "        # of the time you activated the function, and in each call to twitter API it will write in that log-file\n",
    "        # the number of tweets that the function retirved in that call + the totla number of tweets retrived so far.\n",
    "        # With this text file you can follow up in any time the function runs what is the status of the call - \n",
    "        # you can see how many tweets have been collected on a specifc user and know that the function works fine.\n",
    "        path_for_dir_retriving_tweets_streem = os.path.join(path_for_log_dir_of_certain_query, 'retriving_tweets_streem.txt')\n",
    "        with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "            current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "            f.write(current_time + '\\n\\n')\n",
    "        \n",
    "        ############################################################################\n",
    "        ### splitting the tweet_ids into batches\n",
    "        n = number_of_tweets_in_batch\n",
    "        if n > 100:\n",
    "            n = 100\n",
    "            print('Each batch can contain a max of 100 tweet-ids, changed to 100')\n",
    "        if n < 1:\n",
    "            n = 1\n",
    "            print('Each batch must contain a minimum of 1 tweet-id, changed to 1')\n",
    "        \n",
    "        batches=[tweet_ids[i:i + n] for i in range(0, len(tweet_ids), n)]\n",
    "        num_of_returned_tweets = 0 # counter of returned tweets\n",
    "        problematic_batches = [] #save all the batches that weren't evaluated\n",
    "        \n",
    "        from tqdm.notebook import tqdm_notebook\n",
    "        json_response_list = []\n",
    "        json_response_list_all_batches = []\n",
    "        counter_loops = 0\n",
    "        for batch in tqdm_notebook(batches):\n",
    "            print(batch)\n",
    "            \n",
    "            counter_loops +=1\n",
    "            search_url, query_params = self.create_url_tweet_ids(search_url , batch)\n",
    "            try:\n",
    "                json_response = self.__connect_to_endpoint(url = search_url, params= query_params)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Twitter API error: {e} \\n Sleeping 15 min from {datetime.datetime.now()}')\n",
    "                time.sleep(api_error_sleep_secs)\n",
    "                problematic_batches.append(batch)\n",
    "\n",
    "            json_response_list.append(json_response) #the first json_response itme\n",
    "            \n",
    "            ##### making a dataframe out of the json response:\n",
    "            try:\n",
    "                a = pd.json_normalize(json_response[\"data\"])\n",
    "                b = pd.json_normalize(json_response[\"includes\"], [\"users\"]).add_prefix(\"users.\")\n",
    "\n",
    "                a.conversation_id = a.conversation_id.astype(\"string\")\n",
    "                a.id = a.id.astype(\"string\")\n",
    "                a[\"id_new\"] = \"id: \" + a[\"id\"].astype(\"string\")\n",
    "                a[\"conv_id_new\"] = \"conv_id: \" + a[\"conversation_id\"].astype(\"string\")\n",
    "                a[\"author_id_new\"] = \"author_id: \" + a[\"author_id\"].astype(\"string\")\n",
    "\n",
    "            #c = pd.json_normalize(json_response[\"includes\"][\"places\"]).add_prefix(\"places.\")\n",
    "                df_tweets_i = pd.merge(a, b, left_on=\"author_id\", right_on=\"users.id\")\n",
    "                list_of_cols_to_add = ['author_id', \"author_id_new\", 'conversation_id', \"conv_id_new\",\n",
    "                                        \"id\", \"id_new\",'created_at','entities.mentions',\n",
    "                            'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "                        'public_metrics.reply_count', 'public_metrics.retweet_count','referenced_tweets', 'text',\n",
    "                            'users.created_at', 'users.description','users.id', 'users.name',\n",
    "                        'users.public_metrics.followers_count', 'users.public_metrics.following_count',\n",
    "                        'users.public_metrics.listed_count', 'users.public_metrics.tweet_count',\n",
    "                        'users.username', 'users.verified']\n",
    "\n",
    "                list_cols_to_drop = [x for x in df_tweets_i.columns if x not in list_of_cols_to_add]\n",
    "        for batch in tqdm_notebook(batches):\n",
    "\n",
    "                ##droping labels (columns) we don't need\n",
    "                df_tweets_i = df_tweets_i.drop(labels=list_cols_to_drop, axis = 1, errors = \"ignore\")\n",
    "\n",
    "                for col in list_of_cols_to_add:\n",
    "                    if col not in df_tweets_i.columns:\n",
    "                        df_tweets_i[col] = \"NA\"\n",
    "\n",
    "                #sort columns by alphabetic order\n",
    "                col_list_df_tweets_i = df_tweets_i.columns.tolist()\n",
    "                col_list_df_tweets_i.sort()\n",
    "                df_tweets_i = df_tweets_i.reindex(columns=col_list_df_tweets_i)\n",
    "                \n",
    "                num_of_returned_tweets += df_tweets_i.shape[0]\n",
    "                #This will be the csv file name that will contain all the tweets of the specific user:\n",
    "                name =  csv_table_name + \".csv\"\n",
    "                path_for_table = os.path.join(dir_name, name)\n",
    "                if os.path.isfile(path_for_table) == False: #if this is the first table of tweets\n",
    "                    df_tweets_i.to_csv(path_for_table, index=True)\n",
    "                else:\n",
    "                    df_tweets_i.to_csv(path_for_table, mode='a', index=True, header=False)\n",
    "                    \n",
    "                number_of_read_tweets_for_printing = df_tweets_i.shape[0]\n",
    "                \n",
    "                ### document the tweet ids that were succsefuly evaluted:\n",
    "                path_for_evaluated_tweet_ids = os.path.join(path_for_log_dir_of_certain_query, 'evaluated_tweet_ids.txt')\n",
    "                with open(path_for_evaluated_tweet_ids, 'a') as f:\n",
    "                    for tweet_id in batch:\n",
    "                        f.write(tweet_id+'\\n')\n",
    "\n",
    "            except:\n",
    "                print(\"no data / include in the json\")\n",
    "                number_of_read_tweets_for_printing = 0\n",
    "                path_for_not_evaluated_tweet_ids = os.path.join(path_for_log_dir_of_certain_query, 'Problematic_tweet_ids.txt')\n",
    "                with open(path_for_not_evaluated_tweet_ids, 'a') as f:\n",
    "                    for tweet_id in batch:\n",
    "                        f.write(tweet_id+'\\n')\n",
    "            ### save all the json responses in json file:\n",
    "            path_for_dir_all_json_responses = os.path.join(path_for_log_dir_of_certain_query, 'all_json_responses.json')\n",
    "             \n",
    "            if os.path.isfile(path_for_dir_all_json_responses) == False: #if this is the first json of tweets\n",
    "                with open(path_for_dir_all_json_responses, 'w') as outfile:\n",
    "                    json.dump(json_response_list, outfile)\n",
    "            else:\n",
    "                with open(path_for_dir_all_json_responses, 'a') as outfile:\n",
    "                    json.dump(json_response_list, outfile)\n",
    "\n",
    "            ### Writing in the retrieving tweets log file the number of retrieved tweets + status - is the function finished running or keep retrieving more tweets?\n",
    "            with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "                print_stat = str(counter_loops) + \" -> Got from twitter \" + str(number_of_read_tweets_for_printing) + \" tweets, and there are more tweets of that user to get, I am bringing more tweets!\"\n",
    "                f.write(print_stat+'\\n')\n",
    "                print_total = \"Total amount of tweets: \" + str(num_of_returned_tweets)\n",
    "                f.write(print_total+ '\\n\\n')\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        return json_response_list, num_of_returned_tweets, problematic_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f427f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:36:07.638057Z",
     "start_time": "2022-06-18T08:36:07.585087Z"
    }
   },
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "#### return tweets given query\n",
    "\n",
    "\n",
    "def return_tweets_given_tweet_ids_new(self, tweet_ids=[], number_of_tweets_in_batch = 100,\n",
    "                                           verbose_10 = False, dir_name = \"tweets_by_tweet_ids\",\n",
    "                                  csv_table_name = \"brexit_tweet_ids\"):\n",
    "        \"\"\" ## Return Tweets of given query\n",
    "\n",
    "This function enalbes getting all the tweets that follow the given query.\\n\n",
    "\n",
    "+ link to twitter-developer page regarding this capability:\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "\n",
    "+ link to twitter-developer page regarding building a query:\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "### App rate limit:\n",
    "        - App rate limit (Application-only): 900 requests per 15-minute window shared among all users of your app\n",
    "        - User rate limit (User context): 900 requests per 15-minute window per each authenticated user\n",
    "        \n",
    "Parameters\n",
    "- query : str --> You can see the full documentation on how to build a query (what to pass in the `query` argument) in the following link:\\n\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/counts/integrate/build-a-query\n",
    "        \n",
    "- dir_name: by defualy \"tweets_by_tweet_ids\" --> the dir name to put all the data from the function\n",
    "\n",
    "- start_date = The start date is the date that from it the function will look and rerieve tweets.\\n\n",
    "Note that the format of the start time should be: \"2015-12-7T00:00:00Z\" (this is the default)\n",
    "\n",
    "- end_date = The end date is the date that till it the function will look and rerieve tweets.\\n\n",
    "Note that the format of the end time should be: \"2021-12-26T00:00:00Z\" (this is the default)\n",
    "\n",
    "- max_results = The max number of tweets to retrieve in a given call. Must be an integer between 10 to 500.\n",
    "- evaluate_last_token - by default = False--> If you have already retrive dataand you wish to continue retriving from the place you stopped, pass \"True\" to this arguemnt.\n",
    "\n",
    "- limit_amount_of_returned_tweets - by default = 10000000 --> Lets say a given account had 5000 tweets in the given timeframe (from the given start to the given end date), and you wish to get only the first 2000, then pass to this arguemnt 2000. It will automatically stop the function when it reaches the `limit_amount_of_returned_tweets` you provided\n",
    "\n",
    "- verbose_10 - by default = False --> If True then the function will print certain things throught the run of the function.    \n",
    "\n",
    "- csv_table_name - the csv name that you wish to give to the table that the function generates. Note that this name will also be the name of the log dir that will contain the tokens and the tweets-json file. This is important as if you wish to continue searching for more tweets of the same query, you need to provide the same csv table name and of course write \"True\" in the argument: evaluate_last_token\n",
    "        \"\"\"\n",
    "        search_url = \"https://api.twitter.com/2/tweets/?ids=X\" #endpoint use to collect data from\n",
    "        \n",
    "        #creating a directory that will contain the csv file + the log directory of the Key Opinion Leader\n",
    "        import os.path\n",
    "        try:\n",
    "            os.mkdir(dir_name)\n",
    "            print(\"creating directory\", dir_name, \"to insert the tables with the tweets for the given tweet-ids\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_name ,\"already exist\")\n",
    "        \n",
    "    ##### Creating the log directory (that will contain 3 files: The token file, the retriving tweets streem and the full json file)\n",
    "        dir_log_name = os.path.join(dir_name, \"log_tweets\")\n",
    "\n",
    "        # Try creating the directory for the logs, if the path exist don't create a new one. Print in either case What happened\n",
    "        try:\n",
    "            os.mkdir(dir_log_name)\n",
    "            print(\"creating directory\", dir_log_name, \"to insert all the logs of the tweets\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_log_name ,\"already exist\")\n",
    "        ######################## creating a path for the specific user that we wish to retirve his tweets\n",
    "        path_for_log_dir_of_certain_query = os.path.join(dir_log_name, csv_table_name)\n",
    "        # Try creating the directory for the specific user, if the path exist don't create a new one. Print in either case What happened\n",
    "        try:\n",
    "            os.mkdir(path_for_log_dir_of_certain_query)\n",
    "            print(\"creating directory\", path_for_log_dir_of_certain_query,\"in the dir\",dir_log_name, \"to insert all the logs of the QUERY\", csv_table_name)\n",
    "        except:\n",
    "            print(\"The dir\", path_for_log_dir_of_certain_query ,\"already exist\")\n",
    "        ### creating the log-text-file of the \"retriving_tweets_streem\" This log file will contain a time-stamp\n",
    "        # of the time you activated the function, and in each call to twitter API it will write in that log-file\n",
    "        # the number of tweets that the function retirved in that call + the totla number of tweets retrived so far.\n",
    "        # With this text file you can follow up in any time the function runs what is the status of the call - \n",
    "        # you can see how many tweets have been collected on a specifc user and know that the function works fine.\n",
    "        path_for_dir_retriving_tweets_streem = os.path.join(path_for_log_dir_of_certain_query, 'retriving_tweets_streem.txt')\n",
    "        with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "            current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "            f.write(current_time + '\\n\\n')\n",
    "        \n",
    "        ############################################################################\n",
    "        ### splitting the tweet_ids into batches\n",
    "        n = number_of_tweets_in_batch\n",
    "        if n > 100:\n",
    "            n = 100\n",
    "            print('Each batch can contain a max of 100 tweet-ids, changed to 100')\n",
    "        if n < 1:\n",
    "            n = 1\n",
    "            print('Each batch must contain a minimum of 1 tweet-id, changed to 1')\n",
    "        \n",
    "        batches=[tweet_ids[i:i + n] for i in range(0, len(tweet_ids), n)]\n",
    "        num_of_returned_tweets = 0 # counter of returned tweets\n",
    "\n",
    "        for batch in tqdm_notebook(batches):\n",
    "\n",
    "            search_url, query_params = self.create_url_tweet_ids(search_url , tweet_ids_batch)\n",
    "            try:\n",
    "                json_response = self.__connect_to_endpoint( url = search_url, params= query_params)\n",
    "            except Exception as e:\n",
    "                print(f'Twitter API error: {e} \\n Sleeping 15 min from {datetime.datetime.now()}')\n",
    "                time.sleep(api_error_sleep_secs)\n",
    "\n",
    "            json_response_list.append(json_response) #the first json_response itme\n",
    "\n",
    "\n",
    "            ##### making a dataframe out of the json response:\n",
    "            try:\n",
    "                a = pd.json_normalize(json_response[\"data\"])\n",
    "                b = pd.json_normalize(json_response[\"includes\"], [\"users\"]).add_prefix(\"users.\")\n",
    "\n",
    "                a.conversation_id = a.conversation_id.astype(\"string\")\n",
    "                a.id = a.id.astype(\"string\")\n",
    "                a[\"id_new\"] = \"id: \" + a[\"id\"].astype(\"string\")\n",
    "                a[\"conv_id_new\"] = \"conv_id: \" + a[\"conversation_id\"].astype(\"string\")\n",
    "                a[\"author_id_new\"] = \"author_id: \" + a[\"author_id\"].astype(\"string\")\n",
    "\n",
    "            #c = pd.json_normalize(json_response[\"includes\"][\"places\"]).add_prefix(\"places.\")\n",
    "                df_tweets_i = pd.merge(a, b, left_on=\"author_id\", right_on=\"users.id\")\n",
    "                list_of_cols_to_add = ['author_id', \"author_id_new\", 'conversation_id', \"conv_id_new\",\n",
    "                                        \"id\", \"id_new\",'created_at','entities.mentions',\n",
    "                            'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "                        'public_metrics.reply_count', 'public_metrics.retweet_count','referenced_tweets', 'text',\n",
    "                            'users.created_at', 'users.description','users.id', 'users.name',\n",
    "                        'users.public_metrics.followers_count', 'users.public_metrics.following_count',\n",
    "                        'users.public_metrics.listed_count', 'users.public_metrics.tweet_count',\n",
    "                        'users.username', 'users.verified']\n",
    "\n",
    "                list_cols_to_drop = [x for x in df_tweets_i.columns if x not in list_of_cols_to_add]\n",
    "\n",
    "                ##droping labels (columns) we don't need\n",
    "                df_tweets_i = df_tweets_i.drop(labels=list_cols_to_drop, axis = 1, errors = \"ignore\")\n",
    "\n",
    "                for col in list_of_cols_to_add:\n",
    "                    if col not in df_tweets_i.columns:\n",
    "                        df_tweets_i[col] = \"NA\"\n",
    "\n",
    "                #sort columns by alphabetic order\n",
    "                col_list_df_tweets_i = df_tweets_i.columns.tolist()\n",
    "                col_list_df_tweets_i.sort()\n",
    "                df_tweets_i = df_tweets_i.reindex(columns=col_list_df_tweets_i)\n",
    "                \n",
    "                num_of_returned_tweets += df_tweets_i.shape[0]\n",
    "                #This will be the csv file name that will contain all the tweets of the specific user:\n",
    "                name =  csv_table_name + \".csv\"\n",
    "                path_for_table = os.path.join(dir_name, name)\n",
    "                if os.path.isfile(path_for_table) == False: #if this is the first table of tweets\n",
    "                    df_tweets_i.to_csv(path_for_table, index=True)\n",
    "                else:\n",
    "                    df_tweets_i.to_csv(path_for_table, mode='a', index=True, header=False)\n",
    "            except:\n",
    "                print(\"no data / include in the json\")\n",
    "\n",
    "            ### save all the json responses in json file:\n",
    "            path_for_dir_all_json_responses = os.path.join(path_for_log_dir_of_certain_query, 'all_json_responses.json')\n",
    "            with open(path_for_dir_all_json_responses, 'w') as outfile:\n",
    "                json.dump(json_response_list, outfile)\n",
    "\n",
    "            ### Writing in the retrieving tweets log file the number of retrieved tweets + status - is the function finished running or keep retrieving more tweets?\n",
    "            with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "                print_stat = str(counter_loops) + \" -> Got from twitter \" + str(df_tweets_i.shape[0]) + \" tweets, and there are more tweets of that user to get, I am bringing more tweets!\"\n",
    "                f.write(print_stat+'\\n')\n",
    "                print_total = \"Total amount of tweets: \" + str(num_of_returned_tweets)\n",
    "                f.write(print_total+ '\\n\\n')\n",
    "\n",
    "#             if \"next_token\" in json_response[\"meta\"]:\n",
    "#                 if (verbose_10 == True and counter_loops % 20 == 1):\n",
    "#                     print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "#                 #elif verbose_10 == False:\n",
    "#                     #print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "#                 next_token = json_response[\"meta\"][\"next_token\"]\n",
    "#                 query_params[\"next_token\"] = next_token\n",
    "#                 next_tokens.append(next_token)\n",
    "#                 #ids_token_print = \"next token = \" + next_token + \"newest id: \" + json_response[\"meta\"][\"newest_id\"] + \" | oldest id: \" + json_response[\"meta\"][\"oldest_id\"]\n",
    "#                 ids_token_print = next_token\n",
    "#                 with open(path_for_dir_tokens, 'a') as f:\n",
    "#                     f.write(ids_token_print + '\\n\\n')\n",
    "#             else:\n",
    "#                 print(\"no more tweets from this user\")\n",
    "#                 continue_searching = False\n",
    "#                 print(\"Total amount of collected tweets = \", num_of_returned_tweets)\n",
    "\n",
    "#             if num_of_returned_tweets >=limit_amount_of_returned_tweets:\n",
    "#                 print(\"oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\")\n",
    "#                 print(\"infact you got\", num_of_returned_tweets, \"returned tweets and limited the function to get\", limit_amount_of_returned_tweets, \"tweets\")\n",
    "\n",
    "            #In what case we suspect that there may be more tweets that we didn't get? -->\n",
    "            #When the number of tweets we asked to get is equal to the number of tweets we got back\n",
    "#         if user_name is not None:\n",
    "#             query = str(query) + \" from:\" + str(user_name)\n",
    "#             try: #displaying with print the start and end time + the user name that the function will try to retirve tweets\n",
    "#                 display_start_time = datetime.strptime(start_time.split(\"T\")[0], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "#                 display_end_time = datetime.strptime(end_time.split(\"T\")[0], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "#             except:\n",
    "#                 display_start_time = start_time\n",
    "#                 display_end_time = end_time\n",
    "#             print(\"Bringing all the tweets of the user:\", user_name, \"from:\", display_start_time, \"to\", display_end_time)\n",
    "#             print()\n",
    "\n",
    "            \n",
    "\n",
    "        # Opening the Tokens text-file (if not exist)\n",
    "        ########### If the token file exist already, then take the last token available, else start from token 1  ############\n",
    "#         tokens_location = os.path.join(dir_log_name,csv_table_name, \"tokens.txt\")\n",
    "\n",
    "#         if (evaluate_last_token == True and os.path.isfile(tokens_location) == True):\n",
    "#             a_file = open(tokens_location, \"r\")\n",
    "#             lines = a_file.readlines()\n",
    "#             last_lines = lines[-2]\n",
    "#             next_token = last_lines[0:-1]\n",
    "#             a_file.close()\n",
    "#         else:\n",
    "#             next_token = None\n",
    "\n",
    "        ################ Add a time stamp ########################################\n",
    "#         path_for_dir_tokens = os.path.join(path_for_log_dir_of_certain_query, 'tokens.txt')\n",
    "#         with open(path_for_dir_tokens, 'a') as f:\n",
    "#             from datetime import datetime\n",
    "#             now = datetime.now()\n",
    "#             current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "#             current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "#             f.write(current_time+ '\\n\\n')\n",
    "\n",
    "        ############################################# The main for loop #############################################\n",
    "\n",
    "#         continue_searching = True #logical variable that controls the for loop, as long as it is True, the function will keep trying to retrive tweets\n",
    "#         json_response_list = [] #list containing all the json responses the function got. In each loop we are\n",
    "#         #saving that list in the log-json file (so if there was a problem in a certain loop, we would have all the jsons till that problematic loop)\n",
    "#         next_tokens = [] #list containing all the tokens\n",
    "#         num_of_returned_tweets = 0 # counter of returned tweets\n",
    "#         counter_loops = 0 #counter of the loops - the number of calls to twitter API\n",
    "\n",
    "#         while continue_searching == True and num_of_returned_tweets < limit_amount_of_returned_tweets:\n",
    "#             counter_loops +=1\n",
    "\n",
    "#             #change params based on the endpoint you are using\n",
    "#             query_params = {'query': query,\n",
    "#                         'start_time': start_time,\n",
    "#                         'end_time': end_time,\n",
    "#                         'max_results': max_results,\n",
    "#                         'expansions': 'author_id,in_reply_to_user_id,geo.place_id,entities.mentions.username,referenced_tweets.id',\n",
    "#                         'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "#                         'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "#                         'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "#                         'next_token': {next_token}}\n",
    "\n",
    "#             json_response = self.__connect_to_endpoint( url = search_url, params= query_params)\n",
    "\n",
    "            \n",
    "\n",
    "        return json_response_list, num_of_returned_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bbff8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:17:23.260349Z",
     "start_time": "2022-06-18T08:17:23.233362Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_784/3152039423.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjson_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result_count\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'meta'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "#### return tweets given query\n",
    "\n",
    "\n",
    "    def return_tweets_given_query(self, tweet_ids=[],\n",
    "                                           verbose_10 = False, dir_name = \"tweets_tweet_ids\",\n",
    "                                  csv_table_name = \"brexit_tweet_ids\"):\n",
    "        \"\"\" ## Return Tweets of given query\n",
    "\n",
    "This function enalbes getting all the tweets that follow the given query.\\n\n",
    "\n",
    "+ link to twitter-developer page regarding this capability:\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "\n",
    "+ link to twitter-developer page regarding building a query:\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "### App rate limit:\n",
    "        - App rate limit (Application-only): 900 requests per 15-minute window shared among all users of your app\n",
    "        - User rate limit (User context): 900 requests per 15-minute window per each authenticated user\n",
    "        \n",
    "Parameters\n",
    "- query : str --> You can see the full documentation on how to build a query (what to pass in the `query` argument) in the following link:\\n\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/counts/integrate/build-a-query\n",
    "        \n",
    "- dir_name: by defualy \"tweets\" --> the dir name to put all the data from the function\n",
    "\n",
    "- start_date = The start date is the date that from it the function will look and rerieve tweets.\\n\n",
    "Note that the format of the start time should be: \"2015-12-7T00:00:00Z\" (this is the default)\n",
    "\n",
    "- end_date = The end date is the date that till it the function will look and rerieve tweets.\\n\n",
    "Note that the format of the end time should be: \"2021-12-26T00:00:00Z\" (this is the default)\n",
    "\n",
    "- max_results = The max number of tweets to retrieve in a given call. Must be an integer between 10 to 500.\n",
    "- evaluate_last_token - by default = False--> If you have already retrive dataand you wish to continue retriving from the place you stopped, pass \"True\" to this arguemnt.\n",
    "\n",
    "- limit_amount_of_returned_tweets - by default = 10000000 --> Lets say a given account had 5000 tweets in the given timeframe (from the given start to the given end date), and you wish to get only the first 2000, then pass to this arguemnt 2000. It will automatically stop the function when it reaches the `limit_amount_of_returned_tweets` you provided\n",
    "\n",
    "- verbose_10 - by default = False --> If True then the function will print certain things throught the run of the function.    \n",
    "\n",
    "- csv_table_name - the csv name that you wish to give to the table that the function generates. Note that this name will also be the name of the log dir that will contain the tokens and the tweets-json file. This is important as if you wish to continue searching for more tweets of the same query, you need to provide the same csv table name and of course write \"True\" in the argument: evaluate_last_token\n",
    "        \"\"\"\n",
    "        search_url = \"https://api.twitter.com/2/tweets/?ids=X\" #endpoint use to collect data from\n",
    "\n",
    "        \n",
    "         count_proccesed_tweets_file = os.path.join(json_tweets_output_folder, 'count_proccesed_tweets.txt')\n",
    "        count_requests_sent_file = os.path.join(json_tweets_output_folder,'count_requests_sent.txt')\n",
    "        json_output_file_basename = os.path.join(json_tweets_output_folder,'twitter_data_')\n",
    "\n",
    "        if not os.path.exists(count_requests_sent_file):\n",
    "            with open(count_requests_sent_file,'w') as f:\n",
    "                f.write('%d' % 0)\n",
    "\n",
    "        if not os.path.exists(count_proccesed_tweets_file):\n",
    "            with open(count_proccesed_tweets_file,'w') as f:\n",
    "                f.write('%d' % 0)\n",
    "\n",
    "        count_proccesed_tweets = int( open(count_proccesed_tweets_file,'r').read())\n",
    "\n",
    "        count_requests_sent = int( open(count_requests_sent_file,'r').read())\n",
    "\n",
    "        while(count_proccesed_tweets < len(tweet_ids)):\n",
    "\n",
    "            if verbose:\n",
    "                print(f'~~~~~ Already sent {count_requests_sent} requests')\n",
    "                print(f'~~~~~ Already proccesed {count_proccesed_tweets} tweets')\n",
    "                print(f'~~~~~ Currently proccessing {tweets_per_api_request} tweets')\n",
    "\n",
    "\n",
    "            ## FIXME - last batch size cant be more than what's left\n",
    "            if len(tweet_ids)>1:\n",
    "                tweet_ids_batch = tweet_ids[count_proccesed_tweets:(count_proccesed_tweets+ tweets_per_api_request )// (len(tweet_ids)-1)]\n",
    "            else:\n",
    "                tweet_ids_batch = tweet_ids\n",
    "            search_url, query_params = self.create_url_tweet_ids(search_url , tweet_ids_batch)\n",
    "            # headers = create_headers(os.environ['TOKEN'])\n",
    "\n",
    "            try:\n",
    "                json_response = self.__connect_to_endpoint( url = search_url, params= query_params)\n",
    "            except Exception as e:\n",
    "                print(f'Twitter API error: {e} \\n Sleeping 15 min from {datetime.datetime.now()}')\n",
    "                time.sleep(api_error_sleep_secs)\n",
    "\n",
    "\n",
    "            count_proccesed_tweets += tweets_per_api_request\n",
    "            count_requests_sent += 1\n",
    "\n",
    "        \n",
    "        #handling case where the user entered a max_result that is not between 10 and 500\n",
    "        if max_results > 100:\n",
    "            max_results = 100\n",
    "            print('max_results can not be greater than 100, changed to 100')\n",
    "        if max_results < 1:\n",
    "            max_results = 1\n",
    "            print('max_results can not be smaller than 1, changed to 1')\n",
    "        #creating a directory tat will contain the csv file + the log directory of the Key Opinion Leader\n",
    "        import os.path\n",
    "        try:\n",
    "            os.mkdir(dir_name)\n",
    "            print(\"creating directory\", dir_name, \"to insert the table with the tweets for the given tweet-ids\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_name ,\"already exist\")\n",
    "\n",
    "#         if user_name is not None:\n",
    "#             query = str(query) + \" from:\" + str(user_name)\n",
    "#             try: #displaying with print the start and end time + the user name that the function will try to retirve tweets\n",
    "#                 display_start_time = datetime.strptime(start_time.split(\"T\")[0], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "#                 display_end_time = datetime.strptime(end_time.split(\"T\")[0], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "#             except:\n",
    "#                 display_start_time = start_time\n",
    "#                 display_end_time = end_time\n",
    "#             print(\"Bringing all the tweets of the user:\", user_name, \"from:\", display_start_time, \"to\", display_end_time)\n",
    "#             print()\n",
    "\n",
    "            ##### Creating the log directory (that will contain 3 files: The token file, the retriving tweets streem and the full json file)\n",
    "        import os.path\n",
    "        dir_log_name = os.path.join(dir_name, \"log_tweets\")\n",
    "\n",
    "        # Try creating the directory for the logs, if the path exist don't create a new one. Print in either case What happened\n",
    "        try:\n",
    "            os.mkdir(dir_log_name)\n",
    "            print(\"creating directory\", dir_log_name, \"to insert all the logs of the tweets\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_log_name ,\"already exist\")\n",
    "        ######################## creating a path for the specific user that we wish to retirve his tweets\n",
    "        path_for_log_dir_of_certain_query = os.path.join(dir_log_name, csv_table_name)\n",
    "        # Try creating the directory for the specific user, if the path exist don't create a new one. Print in either case What happened\n",
    "        try:\n",
    "            os.mkdir(path_for_log_dir_of_certain_query)\n",
    "            print(\"creating directory\", path_for_log_dir_of_certain_query,\"in the dir\",dir_log_name, \"to insert all the logs of the QUERY\", csv_table_name)\n",
    "        except:\n",
    "            print(\"The dir\", path_for_log_dir_of_certain_query ,\"already exist\")\n",
    "        ### creating the log-text-file of the \"retriving_tweets_streem\" This log file will contain a time-stamp\n",
    "        # of the time you activated the function, and in each call to twitter API it will write in that log-file\n",
    "        # the number of tweets that the function retirved in that call + the totla number of tweets retrived so far.\n",
    "        # With this text file you can follow up in any time the function runs what is the status of the call - \n",
    "        # you can see how many tweets have been collected on a specifc user and know that the function works fine.\n",
    "        path_for_dir_retriving_tweets_streem = os.path.join(path_for_log_dir_of_certain_query, 'retriving_tweets_streem.txt')\n",
    "        with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "            current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "            f.write(current_time + '\\n\\n')\n",
    "\n",
    "        # Opening the Tokens text-file (if not exist)\n",
    "        ########### If the token file exist already, then take the last token available, else start from token 1  ############\n",
    "#         tokens_location = os.path.join(dir_log_name,csv_table_name, \"tokens.txt\")\n",
    "\n",
    "#         if (evaluate_last_token == True and os.path.isfile(tokens_location) == True):\n",
    "#             a_file = open(tokens_location, \"r\")\n",
    "#             lines = a_file.readlines()\n",
    "#             last_lines = lines[-2]\n",
    "#             next_token = last_lines[0:-1]\n",
    "#             a_file.close()\n",
    "#         else:\n",
    "#             next_token = None\n",
    "\n",
    "        ################ Add a time stamp ########################################\n",
    "#         path_for_dir_tokens = os.path.join(path_for_log_dir_of_certain_query, 'tokens.txt')\n",
    "#         with open(path_for_dir_tokens, 'a') as f:\n",
    "#             from datetime import datetime\n",
    "#             now = datetime.now()\n",
    "#             current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "#             current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "#             f.write(current_time+ '\\n\\n')\n",
    "\n",
    "        ############################################# The main for loop #############################################\n",
    "\n",
    "        continue_searching = True #logical variable that controls the for loop, as long as it is True, the function will keep trying to retrive tweets\n",
    "        json_response_list = [] #list containing all the json responses the function got. In each loop we are\n",
    "        #saving that list in the log-json file (so if there was a problem in a certain loop, we would have all the jsons till that problematic loop)\n",
    "        next_tokens = [] #list containing all the tokens\n",
    "        num_of_returned_tweets = 0 # counter of returned tweets\n",
    "        counter_loops = 0 #counter of the loops - the number of calls to twitter API\n",
    "\n",
    "        while continue_searching == True and num_of_returned_tweets < limit_amount_of_returned_tweets:\n",
    "            counter_loops +=1\n",
    "            if counter_loops > 1:\n",
    "                next_token = json_response[\"meta\"][\"next_token\"]\n",
    "                query_params[\"next_token\"] = next_token\n",
    "                #if verbose_10:\n",
    "                #    print(\"token to insert:\",next_token)\n",
    "            #if the returned amount of tweets is getting close to the limit number, we need to alter the max_result,\n",
    "            #so we won't get tweets beyond what we asked\n",
    "            if (limit_amount_of_returned_tweets - num_of_returned_tweets) < max_results:\n",
    "                max_results = limit_amount_of_returned_tweets - num_of_returned_tweets\n",
    "                if max_results < 10:\n",
    "                    max_results = 10\n",
    "            else :\n",
    "                max_results = max_results\n",
    "\n",
    "            #change params based on the endpoint you are using\n",
    "            query_params = {'query': query,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id,entities.mentions.username,referenced_tweets.id',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {next_token}}\n",
    "\n",
    "            json_response = self.__connect_to_endpoint(url = search_url,params= query_params, next_token = next_token)\n",
    "\n",
    "            json_response_list.append(json_response) #the first json_response itme\n",
    "            num_of_returned_tweets += json_response[\"meta\"][\"result_count\"]\n",
    "\n",
    "            ##### making a dataframe out of the json response:\n",
    "            try:\n",
    "                a = pd.json_normalize(json_response[\"data\"])\n",
    "                b = pd.json_normalize(json_response[\"includes\"], [\"users\"]).add_prefix(\"users.\")\n",
    "\n",
    "                a.conversation_id = a.conversation_id.astype(\"string\")\n",
    "                a.id = a.id.astype(\"string\")\n",
    "                a[\"id_new\"] = \"id: \" + a[\"id\"].astype(\"string\")\n",
    "                a[\"conv_id_new\"] = \"conv_id: \" + a[\"conversation_id\"].astype(\"string\")\n",
    "                a[\"author_id_new\"] = \"author_id: \" + a[\"author_id\"].astype(\"string\")\n",
    "\n",
    "            #c = pd.json_normalize(json_response[\"includes\"][\"places\"]).add_prefix(\"places.\")\n",
    "                df_tweets_i = pd.merge(a, b, left_on=\"author_id\", right_on=\"users.id\")\n",
    "                list_of_cols_to_add = ['author_id', \"author_id_new\", 'conversation_id', \"conv_id_new\",\n",
    "                                        \"id\", \"id_new\",'created_at','entities.mentions',\n",
    "                            'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "                        'public_metrics.reply_count', 'public_metrics.retweet_count','referenced_tweets', 'text',\n",
    "                            'users.created_at', 'users.description','users.id', 'users.name',\n",
    "                        'users.public_metrics.followers_count', 'users.public_metrics.following_count',\n",
    "                        'users.public_metrics.listed_count', 'users.public_metrics.tweet_count',\n",
    "                        'users.username', 'users.verified']\n",
    "\n",
    "                list_cols_to_drop = [x for x in df_tweets_i.columns if x not in list_of_cols_to_add]\n",
    "\n",
    "                ##droping labels (columns) we don't need\n",
    "                df_tweets_i = df_tweets_i.drop(labels=list_cols_to_drop, axis = 1, errors = \"ignore\")\n",
    "\n",
    "                for col in list_of_cols_to_add:\n",
    "                    if col not in df_tweets_i.columns:\n",
    "                        df_tweets_i[col] = \"NA\"\n",
    "\n",
    "                #sort columns by alphabetic order\n",
    "                col_list_df_tweets_i = df_tweets_i.columns.tolist()\n",
    "                col_list_df_tweets_i.sort()\n",
    "                df_tweets_i = df_tweets_i.reindex(columns=col_list_df_tweets_i)\n",
    "\n",
    "                #This will be the csv file name that will contain all the tweets of the specific user:\n",
    "                name =  csv_table_name + \".csv\"\n",
    "                path_for_table = os.path.join(dir_name, name)\n",
    "                if os.path.isfile(path_for_table) == False: #if this is the first table of tweets\n",
    "                    df_tweets_i.to_csv(path_for_table, index=True)\n",
    "                else:\n",
    "                    df_tweets_i.to_csv(path_for_table, mode='a', index=True, header=False)\n",
    "            except:\n",
    "                print(\"no data / include in the json\")\n",
    "\n",
    "            ### save all the json responses in json file:\n",
    "            path_for_dir_all_json_responses = os.path.join(path_for_log_dir_of_certain_query, 'all_json_responses.json')\n",
    "            with open(path_for_dir_all_json_responses, 'w') as outfile:\n",
    "                json.dump(json_response_list, outfile)\n",
    "\n",
    "            ### Writing in the retrieving tweets log file the number of retrieved tweets + status - is the function finished running or keep retrieving more tweets?\n",
    "            with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "                print_stat = str(counter_loops) + \" -> Got from twitter \" + str(json_response[\"meta\"][\"result_count\"]) + \" tweets, and there are more tweets of that user to get, I am bringing more tweets!\"\n",
    "                f.write(print_stat+'\\n')\n",
    "                print_total = \"Total amount of tweets: \" + str(num_of_returned_tweets)\n",
    "                f.write(print_total+ '\\n\\n')\n",
    "\n",
    "            if \"next_token\" in json_response[\"meta\"]:\n",
    "                if (verbose_10 == True and counter_loops % 20 == 1):\n",
    "                    print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "                #elif verbose_10 == False:\n",
    "                    #print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "                next_token = json_response[\"meta\"][\"next_token\"]\n",
    "                query_params[\"next_token\"] = next_token\n",
    "                next_tokens.append(next_token)\n",
    "                #ids_token_print = \"next token = \" + next_token + \"newest id: \" + json_response[\"meta\"][\"newest_id\"] + \" | oldest id: \" + json_response[\"meta\"][\"oldest_id\"]\n",
    "                ids_token_print = next_token\n",
    "                with open(path_for_dir_tokens, 'a') as f:\n",
    "                    f.write(ids_token_print + '\\n\\n')\n",
    "            else:\n",
    "                print(\"no more tweets from this user\")\n",
    "                continue_searching = False\n",
    "                print(\"Total amount of collected tweets = \", num_of_returned_tweets)\n",
    "\n",
    "            if num_of_returned_tweets >=limit_amount_of_returned_tweets:\n",
    "                print(\"oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\")\n",
    "                print(\"infact you got\", num_of_returned_tweets, \"returned tweets and limited the function to get\", limit_amount_of_returned_tweets, \"tweets\")\n",
    "\n",
    "            #In what case we suspect that there may be more tweets that we didn't get? -->\n",
    "            #When the number of tweets we asked to get is equal to the number of tweets we got back\n",
    "\n",
    "        return json_response_list, num_of_returned_tweets, next_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81455b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
