{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf22d0d",
   "metadata": {},
   "source": [
    "### Step_0 Retrieve tweets for brexit\n",
    "In this notebook we retrive from twitter tweets that are relevant to brexit from different time-frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da172a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:30:25.255474Z",
     "start_time": "2022-06-18T08:30:20.604710Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv , datetime, unicodedata, time, tweeterid #dateutil.parserm,\n",
    "import gensim\n",
    "import openpyxl\n",
    "import json\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from inputimeout import inputimeout, TimeoutOccurred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b74142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:30:25.502334Z",
     "start_time": "2022-06-18T08:30:25.484352Z"
    }
   },
   "outputs": [],
   "source": [
    "from Brexit_Package import TwitterCrawler\n",
    "\n",
    "my_token = 'NLP_is_fun' ### Of course one need to provide here a real valid token, unfortunelty we can't publish our token...\n",
    "#my_token = \"AAAAAAAAAAAAAAAAAAAAAPr2WAEAAAAARCTqXt1KnbCbnG4FzY8S8A5zoYg%3DYWyy0y9SQtwDenwjkU8UB662tfhm8yxZl3Ud3T3aEerCMT6B9W\" #roy madpis\n",
    "my_token = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT' #idc\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892e352c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:30:25.706221Z",
     "start_time": "2022-06-18T08:30:25.695224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "limit_amount_of_returned_tweets = 25000\n",
    "max_results = 500\n",
    "dir_name = \"Roy_tweets_brexit\"\n",
    "query = \"(Brexit OR BREXIT OR brexit OR BRexit OR BREX OR brex OR brexi OR Brexi) lang:en -is:retweet -is:reply -is:quote -from:theresa_may -from:NicolaSturgeon -from:MichelBarnier -from:fhollande -from:PhilipHammondUK -from:DavidDavisMP -from:JunckerEU -from:guyverhofstadt -from:EndaKennyTD -from:hilarybennmp -from:MinPres -from:MartinSelmayr -from:Keir_Starmer -from:LiamFox -from:TimmermansEU -from:BorisJohnson -from:Nigel_Farage -from:ManfredWeber -from:davidmcallister -from:matteorenzi -from:SadiqKhan -from:cbicarolyn -from:rupertmurdoch -from:WeyandSabine -from:MLP_officiel -from:FrancesOGrady -from:giannipittella -from:MalmstromEU -from:TheGinaMiller -from:CER_Grant -from:AMCarwyn -from:AlunCairns -from:DominicRaab  -from:michaelgove  -from:theJeremyVine -from:George_Osborne  -from:michaelhsltine_  -from:WilliamJHague -from:ChukaUmunna -from:jeremycorbyn -from:Dyson -from:SteveO_Connell -from:MorrisseyHelena -from:GroovyTimbo\"\n",
    " \n",
    "\n",
    "print(limit_amount_of_returned_tweets*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d9304",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0667c90f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T11:36:30.966871Z",
     "start_time": "2022-06-17T11:36:10.002695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2014 already exist\n",
      "no more tweets from this user\n",
      "Total amount of collected tweets =  4252\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2014-01-01T00:00:00Z\"\n",
    "end_time = \"2014-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2014\"\n",
    "limit_amount_of_returned_tweets = 40000\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = False, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc10f9",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687a30f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:02:02.996615Z",
     "start_time": "2022-06-17T11:56:03.878320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2015 already exist\n",
      "1 Got from twitter 487 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 490 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 494 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25002 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2015-01-01T00:00:00Z\"\n",
    "end_time = \"2015-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2015\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62206657",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fcc5c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:07:09.721477Z",
     "start_time": "2022-06-17T12:02:33.530666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2016 already exist\n",
      "1 Got from twitter 495 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 493 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 498 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25009 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2016-01-01T00:00:00Z\"\n",
    "end_time = \"2016-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2016\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94327ba6",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc6e064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:13:40.392640Z",
     "start_time": "2022-06-17T12:07:32.978863Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2017 already exist\n",
      "1 Got from twitter 489 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 488 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 492 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25004 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2017-01-01T00:00:00Z\"\n",
    "end_time = \"2017-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2017\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f066e1",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b457f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:19:50.996707Z",
     "start_time": "2022-06-17T12:14:05.541970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2018 already exist\n",
      "1 Got from twitter 489 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 493 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 482 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25003 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2018-01-01T00:00:00Z\"\n",
    "end_time = \"2018-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2018\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb160cd3",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4541d87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:26:13.731308Z",
     "start_time": "2022-06-17T12:20:32.425998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2019 already exist\n",
      "1 Got from twitter 484 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 490 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 478 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25007 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2019-01-01T00:00:00Z\"\n",
    "end_time = \"2019-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2019\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb8e4f",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc46d00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:30:55.372093Z",
     "start_time": "2022-06-17T12:26:38.733891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2020 already exist\n",
      "1 Got from twitter 487 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 482 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 487 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25001 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2020-01-01T00:00:00Z\"\n",
    "end_time = \"2020-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2020\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587630dc",
   "metadata": {},
   "source": [
    "### Retrive tweets from 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cf2c94c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:36:43.505644Z",
     "start_time": "2022-06-17T12:31:23.882169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir Roy_tweets_brexit already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets already exist\n",
      "The dir Roy_tweets_brexit\\log_tweets\\brexit_2021 already exist\n",
      "1 Got from twitter 497 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "21 Got from twitter 494 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "41 Got from twitter 490 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 25008 returned tweets and limited the function to get 25000 tweets\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2021-01-01T00:00:00Z\"\n",
    "end_time = \"2021-12-31T00:00:00Z\"\n",
    "csv_table_name = \"brexit_2021\"\n",
    "\n",
    "###########################################\n",
    "json_response_list, num_of_returned_tweets, next_tokens  = my_twitter_crawler.return_tweets_given_query(\n",
    "    query=query,\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    max_results = max_results, evaluate_last_token = True,\n",
    "    limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "   verbose_10 = True, dir_name =dir_name, csv_table_name = csv_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7a5d2",
   "metadata": {},
   "source": [
    "## Retrieve tweets given tweet-id\n",
    "We have 4.5M tweet-ids with classification for positive/negative/neutral tweets,\n",
    "We wish to read them from twitter using the Brexit Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6e6b1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:09:59.056203Z",
     "start_time": "2022-06-18T07:07:13.181003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c52d34054c4067b06d08e36efc13c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 6: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1424/289699957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msentiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation_of_txt_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mtweet_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1255.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 6: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "### Reading all the tweet-ids + the sentiment and save those in a table\n",
    "location_of_txt_files = \"nlp-project/data_folder/tweets_4_5M/Brexit_tweets_stance\"\n",
    "\n",
    "#tqdm_notebook(filtered_table.clean_text):\n",
    "df_all = pd.DataFrame({\"tweet_id\":[0], \"sentimnt\":[0]})\n",
    "\n",
    "for i, file in tqdm_notebook(enumerate(os.listdir(location_of_txt_files))):\n",
    "    tweet_ids= []\n",
    "    sentiments = []\n",
    "    try:\n",
    "        with open(os.path.join(location_of_txt_files, file)) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                tweet_id, sentiment = line.split()\n",
    "                tweet_ids.append(tweet_id)\n",
    "                sentiments.append(sentiment)\n",
    "            df = pd.DataFrame({\"tweet_id\":tweet_ids, \"sentimnt\":sentiments})\n",
    "            df_all = pd.concat([df_all, df])\n",
    "    except:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4dddd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T08:29:08.085440Z",
     "start_time": "2022-06-18T08:29:08.070471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentimnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>730759428430008320</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>730759413938655232</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>730759400445644801</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>730759393965412352</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>730759392925261824</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>730759383253192705</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>730759377548918784</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>730759375174897664</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>730759372603789313</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>730759366530478080</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id  sentimnt\n",
       "0           0  730759428430008320  Positive\n",
       "1           1  730759413938655232  Negative\n",
       "2           2  730759400445644801  Positive\n",
       "3           3  730759393965412352   Neutral\n",
       "4           4  730759392925261824  Positive\n",
       "5           5  730759383253192705   Neutral\n",
       "6           6  730759377548918784  Negative\n",
       "7           7  730759375174897664   Neutral\n",
       "8           8  730759372603789313  Negative\n",
       "9           9  730759366530478080   Neutral"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35b6a901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:15:14.031592Z",
     "start_time": "2022-06-18T07:15:12.892246Z"
    }
   },
   "outputs": [],
   "source": [
    "### save all the data in a file\n",
    "df_all = df_all[df_all[\"tweet_id\"]!=0]\n",
    "print(\"number of tweets:\", df_all.shape[0])\n",
    "df_all = df_all.drop_duplicates(subset=[\"tweet_id\"])\n",
    "print(\"number of tweets after removing duplicate ids:\", df_all.shape[0])\n",
    "print(\"Saving the table in a csv file\")\n",
    "df_all.to_csv(os.path.join(\"data_folder/tweets_4_5M\", \"all_tweetids_and_semtiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f77e8e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:27:03.213988Z",
     "start_time": "2022-06-18T09:26:59.540049Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_all= pd.read_csv(\"data_folder/tweets_4_5M/all_tweetids_and_semtiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e404be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T07:17:58.250256Z",
     "start_time": "2022-06-18T07:17:58.243263Z"
    }
   },
   "source": [
    "# Reading the tweets by tweet id and add the sentiment that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10e2467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:13:45.402258Z",
     "start_time": "2022-06-18T10:13:41.545657Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv , datetime, unicodedata, time, tweeterid #dateutil.parserm,\n",
    "import gensim\n",
    "import openpyxl\n",
    "import json\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from inputimeout import inputimeout, TimeoutOccurred\n",
    "\n",
    "from Brexit_Package import TwitterCrawler\n",
    "\n",
    "my_token = 'NLP_is_fun' ### Of course one need to provide here a real valid token, unfortunelty we can't publish our token...\n",
    "#my_token = \"AAAAAAAAAAAAAAAAAAAAAPr2WAEAAAAARCTqXt1KnbCbnG4FzY8S8A5zoYg%3DYWyy0y9SQtwDenwjkU8UB662tfhm8yxZl3Ud3T3aEerCMT6B9W\" #roy madpis\n",
    "my_token = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT' #idc\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db37844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:44:12.684763Z",
     "start_time": "2022-06-18T10:44:08.343919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets we have sentiment labels on them: 5499483\n"
     ]
    }
   ],
   "source": [
    "### reading the table with all the tweets ids we know their sentiment, there are about 5M tweets\n",
    "df_all= pd.read_csv(\"data_folder/tweets_4_5M/all_tweetids_and_semtiment.csv\")\n",
    "print(\"Number of tweets we have sentiment labels on them:\", df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f601a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:44:28.634728Z",
     "start_time": "2022-06-18T10:44:28.520790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "### we will not export all those 5M tweets, we will retrive 100K of them\n",
    "total_amount_of_tweets_we_want = 100000\n",
    "tweet_ids_n = list(df_all.tweet_id[0:total_amount_of_tweets_we_want])\n",
    "print(len(tweet_ids_n))\n",
    "\n",
    "### turn the tweet-ids from numbers to strings\n",
    "tweet_ids = []\n",
    "for i in tweet_ids_n:\n",
    "    tweet_ids.append(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40e484",
   "metadata": {},
   "source": [
    "### check which tweet ids were already evaluated\n",
    "In this way we won't read the same tweet-id twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4318e415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:45:01.609627Z",
     "start_time": "2022-06-18T10:45:01.489696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir tweets_by_tweet_ids already exist\n",
      "There are 90000 tweet ids to search; see here the first 20:\n",
      "['731573541964021760', '731409988942479360', '731608769713586176', '731233111980294144', '731165221629378561', '730910784662949888', '730907143415681026', '731126880691204096', '731351703455469568', '731157095366705153', '731507459303604224', '731410074535636992', '731405344220565504', '731243652043919360', '731599855597060096', '731039357512089600', '731026417786474496', '731159984931737601', '731082378014294016', '731187202391166976']\n",
      "\n",
      "There are 10000 Tweet ids we already have data on --> here are the first 20:\n",
      "['730759365142167552', '730759363175034880', '730759334796349440', '730759346766876672', '730759344879505409', '730759317884915712', '730759310754611201', '730759317075439617', '730759295478960129', '730759305792749568', '730759304253452289', '730759296955367424', '730759278630424577', '730759264386617344', '730759228223279104', '730759251883347968', '730759256530661377', '730759212268052480', '730759244979396608', '730759212163305472']\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##################################\n",
    "all_tweet_ids = tweet_ids\n",
    "dir_name = \"tweets_by_tweet_ids\"\n",
    "csv_table_name = \"brexit_tweet_ids\"\n",
    "\n",
    "##############################################################################################3\n",
    "#### we don't want to get data on tweet-ids that we already searched for their tweets!\n",
    "# thus we first need to see which of the tweet-ids that are inserted in the list are **not** in the log-file we already have:\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os.path\n",
    "try:\n",
    "    os.mkdir(dir_name)\n",
    "    print(\"creating directory\", dir_name, \"to insert all the tables of all the tweet-idss\")\n",
    "except:\n",
    "    print(\"The dir\", dir_name ,\"already exist\")\n",
    "\n",
    "location_of_log_tweetids_txt_file = os.path.join(dir_name, \"log_tweets\", csv_table_name, \"evaluated_tweet_ids.txt\")\n",
    "\n",
    "#tqdm_notebook(filtered_table.clean_text):\n",
    "evaluated_tweet_ids = []\n",
    "try:\n",
    "    with open(location_of_log_tweetids_txt_file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tweet_id = line.split()\n",
    "            evaluated_tweet_ids.append(tweet_id[0])        \n",
    "except:\n",
    "    print(\"file not existed\")\n",
    "\n",
    "tweet_ids_we_have_data = evaluated_tweet_ids\n",
    "tweet_ids_to_search = list(set(all_tweet_ids) - set(tweet_ids_we_have_data))\n",
    "    \n",
    "if len(tweet_ids_to_search) > 20:\n",
    "    print(\"There are\", len(tweet_ids_to_search), \"tweet ids to search; see here the first 20:\")\n",
    "    print(tweet_ids_to_search[0:20])\n",
    "else:\n",
    "    print(\"Tweet ids to search:\",tweet_ids_to_search)\n",
    "    print(\"-----------------------------------------------\")\n",
    "print()\n",
    "if len(tweet_ids_we_have_data) > 20:\n",
    "    print(\"There are\", len(tweet_ids_we_have_data), \"Tweet ids we already have data on --> here are the first 20:\")\n",
    "    print(tweet_ids_we_have_data[0:20])\n",
    "else : print(\"Tweet-ids we already have data on:\",list(set(all_tweet_ids).intersection(tweet_ids_we_have_data)))\n",
    "\n",
    "#print(\"Tweet-ids to search:\",tweet_ids_to_search)\n",
    "print(\"-----------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54e28e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-18T10:45:11.413Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dir tweets_by_tweet_ids already exist\n",
      "The dir tweets_by_tweet_ids\\log_tweets already exist\n",
      "The dir tweets_by_tweet_ids\\log_tweets\\brexit_tweet_ids already exist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f111a8d4333b4835a0fbc20858e551c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_response_list, num_of_returned_tweets, problematic_batches = my_twitter_crawler.return_tweets_given_tweet_ids_new(\n",
    "    tweet_ids=tweet_ids_to_search,\n",
    "    number_of_tweets_in_batch = 100,\n",
    "    verbose_10 = True, dir_name = dir_name,\n",
    "    csv_table_name = csv_table_name,\n",
    "    api_error_sleep_secs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81d649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T09:51:10.324061Z",
     "start_time": "2022-06-18T09:51:10.307074Z"
    }
   },
   "source": [
    "## Combining the sentiment table with the tweet we exported from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab8e49c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:43:29.467473Z",
     "start_time": "2022-06-18T10:43:21.291315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets we read from twitter: 5334\n",
      "Number of tweets with sentiment: 5334\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_id_new</th>\n",
       "      <th>conv_id_new</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities.mentions</th>\n",
       "      <th>id</th>\n",
       "      <th>id_new</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>users.id</th>\n",
       "      <th>users.name</th>\n",
       "      <th>users.public_metrics.followers_count</th>\n",
       "      <th>users.public_metrics.following_count</th>\n",
       "      <th>users.public_metrics.listed_count</th>\n",
       "      <th>users.public_metrics.tweet_count</th>\n",
       "      <th>users.username</th>\n",
       "      <th>users.verified</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentimnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137099503</td>\n",
       "      <td>author_id: 137099503</td>\n",
       "      <td>conv_id: 730759365142167552</td>\n",
       "      <td>730759365142167552</td>\n",
       "      <td>2016-05-12T13:59:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759365142167552</td>\n",
       "      <td>id: 730759365142167552</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137099503</td>\n",
       "      <td>Fony Blair</td>\n",
       "      <td>463</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>22398</td>\n",
       "      <td>FonyBlair</td>\n",
       "      <td>False</td>\n",
       "      <td>730759365142167552</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367718336</td>\n",
       "      <td>author_id: 367718336</td>\n",
       "      <td>conv_id: 730759317884915712</td>\n",
       "      <td>730759317884915712</td>\n",
       "      <td>2016-05-12T13:59:32.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759317884915712</td>\n",
       "      <td>id: 730759317884915712</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>367718336</td>\n",
       "      <td>Janice Morphet</td>\n",
       "      <td>3623</td>\n",
       "      <td>5000</td>\n",
       "      <td>433</td>\n",
       "      <td>301390</td>\n",
       "      <td>janicemorphet</td>\n",
       "      <td>False</td>\n",
       "      <td>730759317884915712</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1299769218</td>\n",
       "      <td>author_id: 1299769218</td>\n",
       "      <td>conv_id: 730759295478960129</td>\n",
       "      <td>730759295478960129</td>\n",
       "      <td>2016-05-12T13:59:27.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759295478960129</td>\n",
       "      <td>id: 730759295478960129</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1299769218</td>\n",
       "      <td>Kay Burley</td>\n",
       "      <td>558874</td>\n",
       "      <td>359</td>\n",
       "      <td>1630</td>\n",
       "      <td>81943</td>\n",
       "      <td>KayBurley</td>\n",
       "      <td>True</td>\n",
       "      <td>730759295478960129</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2772884435</td>\n",
       "      <td>author_id: 2772884435</td>\n",
       "      <td>conv_id: 730759278630424577</td>\n",
       "      <td>730759278630424577</td>\n",
       "      <td>2016-05-12T13:59:23.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759278630424577</td>\n",
       "      <td>id: 730759278630424577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2772884435</td>\n",
       "      <td>Risethefyrd</td>\n",
       "      <td>1206</td>\n",
       "      <td>1415</td>\n",
       "      <td>25</td>\n",
       "      <td>31524</td>\n",
       "      <td>ernieharding59</td>\n",
       "      <td>False</td>\n",
       "      <td>730759278630424577</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2867356894</td>\n",
       "      <td>author_id: 2867356894</td>\n",
       "      <td>conv_id: 730759264386617344</td>\n",
       "      <td>730759264386617344</td>\n",
       "      <td>2016-05-12T13:59:20.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>730759264386617344</td>\n",
       "      <td>id: 730759264386617344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2867356894</td>\n",
       "      <td>Dr C L Spillard</td>\n",
       "      <td>2922</td>\n",
       "      <td>2474</td>\n",
       "      <td>122</td>\n",
       "      <td>64418</td>\n",
       "      <td>CandiSpillard</td>\n",
       "      <td>False</td>\n",
       "      <td>730759264386617344</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id          author_id_new                  conv_id_new  \\\n",
       "0   137099503   author_id: 137099503  conv_id: 730759365142167552   \n",
       "1   367718336   author_id: 367718336  conv_id: 730759317884915712   \n",
       "2  1299769218  author_id: 1299769218  conv_id: 730759295478960129   \n",
       "3  2772884435  author_id: 2772884435  conv_id: 730759278630424577   \n",
       "4  2867356894  author_id: 2867356894  conv_id: 730759264386617344   \n",
       "\n",
       "      conversation_id                created_at  entities.mentions  \\\n",
       "0  730759365142167552  2016-05-12T13:59:44.000Z                NaN   \n",
       "1  730759317884915712  2016-05-12T13:59:32.000Z                NaN   \n",
       "2  730759295478960129  2016-05-12T13:59:27.000Z                NaN   \n",
       "3  730759278630424577  2016-05-12T13:59:23.000Z                NaN   \n",
       "4  730759264386617344  2016-05-12T13:59:20.000Z                NaN   \n",
       "\n",
       "                   id                  id_new  public_metrics.like_count  \\\n",
       "0  730759365142167552  id: 730759365142167552                          2   \n",
       "1  730759317884915712  id: 730759317884915712                          0   \n",
       "2  730759295478960129  id: 730759295478960129                          4   \n",
       "3  730759278630424577  id: 730759278630424577                          0   \n",
       "4  730759264386617344  id: 730759264386617344                          0   \n",
       "\n",
       "   public_metrics.quote_count  ...    users.id       users.name  \\\n",
       "0                           0  ...   137099503       Fony Blair   \n",
       "1                           0  ...   367718336   Janice Morphet   \n",
       "2                           0  ...  1299769218       Kay Burley   \n",
       "3                           0  ...  2772884435      Risethefyrd   \n",
       "4                           0  ...  2867356894  Dr C L Spillard   \n",
       "\n",
       "  users.public_metrics.followers_count users.public_metrics.following_count  \\\n",
       "0                                  463                                   71   \n",
       "1                                 3623                                 5000   \n",
       "2                               558874                                  359   \n",
       "3                                 1206                                 1415   \n",
       "4                                 2922                                 2474   \n",
       "\n",
       "  users.public_metrics.listed_count users.public_metrics.tweet_count  \\\n",
       "0                                18                            22398   \n",
       "1                               433                           301390   \n",
       "2                              1630                            81943   \n",
       "3                                25                            31524   \n",
       "4                               122                            64418   \n",
       "\n",
       "   users.username users.verified            tweet_id  sentimnt  \n",
       "0       FonyBlair          False  730759365142167552   Neutral  \n",
       "1   janicemorphet          False  730759317884915712   Neutral  \n",
       "2       KayBurley           True  730759295478960129   Neutral  \n",
       "3  ernieharding59          False  730759278630424577  Positive  \n",
       "4   CandiSpillard          False  730759264386617344  Negative  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read dataframe with all the tweet-ids and the data on them - that we exported from twitter\n",
    "tweets_no_sentiment = pd.read_csv(os.path.join(dir_name,csv_table_name + \".csv\"))\n",
    "\n",
    "### cleaning the data and remove duplicates\n",
    "tweets_no_sentiment = tweets_no_sentiment.drop(labels = [\"Unnamed: 0\"], axis = 1)\n",
    "tweets_no_sentiment = tweets_no_sentiment.drop_duplicates(subset=[\"id\", \"created_at\"])\n",
    "print(\"Number of tweets we read from twitter:\", tweets_no_sentiment.shape[0])\n",
    "\n",
    "### read dataframe with all the tweet-ids and the sentiment\n",
    "df_all= pd.read_csv(\"data_folder/tweets_4_5M/all_tweetids_and_semtiment.csv\")\n",
    "df_all = df_all.drop(labels = [\"Unnamed: 0\"], axis = 1)\n",
    "\n",
    "### Merging the two tables so we will have one table with the tweets data + sentiment labels\n",
    "tweets_with_sentiment = pd.merge(left = tweets_no_sentiment, right = df_all, how = \"inner\", left_on='id', right_on='tweet_id')\n",
    "print(\"Number of tweets with sentiment:\", tweets_with_sentiment.shape[0])\n",
    "\n",
    "tweets_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c81455b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T10:43:54.333393Z",
     "start_time": "2022-06-18T10:43:53.563906Z"
    }
   },
   "outputs": [],
   "source": [
    "### writing the table as csv\n",
    "tweets_with_sentiment.to_csv(os.path.join(\"data_folder/tweets_4_5M\", \"tweets_with_sentiment_roy_approve.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4bf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
