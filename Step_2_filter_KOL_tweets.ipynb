{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04b1271",
   "metadata": {},
   "source": [
    "## Step 2 - Filtering Key Opinion Leaders Tweets\n",
    "\n",
    "In this step we are filtering the tweets of the Key Opinion Leaders, that we retrieved in the first step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ddb502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inputimeout\n",
      "  Downloading inputimeout-1.0.4-py3-none-any.whl (4.6 kB)\n",
      "Installing collected packages: inputimeout\n",
      "Successfully installed inputimeout-1.0.4\n"
     ]
    }
   ],
   "source": [
    "#! pip install inputimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2bdd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:37:22.151769Z",
     "start_time": "2022-06-17T12:37:21.904912Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv , datetime, unicodedata, time, tweeterid #dateutil.parserm,\n",
    "import gensim\n",
    "import openpyxl\n",
    "import json\n",
    "\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from inputimeout import inputimeout, TimeoutOccurred\n",
    "\n",
    "data_folder_name = \"data_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53af82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==4.1.2\n",
      "  Using cached gensim-4.1.2-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\roymad\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\roymad\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (0.29.23)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\roymad\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\roymad\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.21.5)\n",
      "Installing collected packages: gensim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\roymad\\\\Anaconda3\\\\Lib\\\\site-packages\\\\gensim\\\\_matutils.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#! pip install gensim==4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483acb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48fb5a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T12:39:30.864505Z",
     "start_time": "2022-06-17T12:39:30.831526Z"
    }
   },
   "outputs": [],
   "source": [
    "from Brexit_Package import TwitterCrawler\n",
    "\n",
    "my_token = 'NLP_is_fun' ### Of course one need to provide here a real valid token, unfortunelty we can't publish our token...\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a6838",
   "metadata": {},
   "source": [
    "### Read all the data we have = all the tweets we read from twitter + some Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184358e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T13:28:58.239452Z",
     "start_time": "2022-06-17T13:18:15.734325Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading all the csv files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roymad\\AppData\\Local\\Temp\\ipykernel_39372\\2557764432.py:30: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_tables.append(pd.read_csv(file_location))\n",
      "C:\\Users\\roymad\\AppData\\Local\\Temp\\ipykernel_39372\\2557764432.py:30: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_tables.append(pd.read_csv(file_location))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  1362559 Tweets\n",
      "There are:  979821 Non duplicates Tweets\n",
      "Step 4-A: Reading stop words data and adding your stop words\n",
      "\n",
      "Step 4-B: Preprocess the tweets table - the text column - using contractions \n",
      "\n",
      "Step 5: Preprocess the tweets table for scoring:\n",
      "Removing Stop words| bigram, trigram, forthgram | merging the tweets table with the KOP and Events tables | \n",
      "found 95256 bigrams\n",
      "found 109745 trigram\n",
      "found 87388 forthgram\n",
      "\n",
      "Finish preprocessing the tweets table, it took: 142.882 seconds\n",
      "\n",
      "Finish!\n",
      "Total time: 200.017 Seconds ( 3.334 Minutes)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "dir_with_all_tweets = os.path.join(\"KOL tweets\")\n",
    "stop_words_file_name = \"stopwords.txt\"\n",
    "\n",
    "score_for_KOP = 0\n",
    "score_for_key_event = 5\n",
    "score_for_key_words = 5\n",
    "\n",
    "key_words = [\"brexit\", \"eu\", \"deal\", \"economy\"]\n",
    "threshold_score = 5\n",
    "verbose = True\n",
    "\n",
    "KOP_excel_name = \"KOP brexit.xlsx\"\n",
    "key_events_excel_name = \"Brexit_key_events.xlsx\"\n",
    "\n",
    "stop_words_to_add = [\"http\", \"https\", \"rt\", \"co\", \"vrkhaxde\", \"oeblog\", \"rln\", \"simonjhix\", \"cctvqfr\",\n",
    "\"dcpj\", \"xvy\", \"mycekvwlxr\", \"imydbvvwji\", \"kkd\", \"rwp\",\"yfc\",\"fus\",\"tmawgoafhb\",\"edzmidvpel\"]\n",
    "\n",
    "start_fun_time = time.time()\n",
    "############# step 1 - Reading the tweets data #############\n",
    "if verbose: print(\"Step 1: Reading all the csv files\")\n",
    "tweets_tables = []\n",
    "csv_files_evaluated = []\n",
    "for root,dirs,files in os.walk(dir_with_all_tweets):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"): #if the file is csv\n",
    "            csv_files_evaluated.append(file)\n",
    "            #print(file)\n",
    "            file_location = os.path.join(dir_with_all_tweets, file)\n",
    "            tweets_tables.append(pd.read_csv(file_location))\n",
    "\n",
    "tweets_table = pd.concat(tweets_tables)\n",
    "tweets_table = tweets_table.drop(labels = [\"Unnamed: 0\"], axis = 1)\n",
    "all_tweets_back_up = tweets_table.copy()\n",
    "if verbose: print(\"There are: \", tweets_table.shape[0], \"Tweets\")\n",
    "    \n",
    "### removing duplicates rows\n",
    "tweets_table = tweets_table.drop_duplicates(subset=['author_id_new', 'conv_id_new', \"id_new\", \"created_at\", \"public_metrics.like_count\", \"text\"])\n",
    "if verbose: print(\"There are: \", tweets_table.shape[0], \"Non duplicates Tweets\")\n",
    "    \n",
    "# ############# step 2 - Reading events data #############\n",
    "# if verbose: print(\"Step 2: Reading The Events Excel File\")\n",
    "# dir_path_for_events_table = os.path.join(key_events_excel_name)\n",
    "# events_table = pd.read_excel(dir_path_for_events_table)\n",
    "# events_table = events_table.drop(labels = [\"tweets\", \"Arguments\", \"Index\"], axis = 1)\n",
    "# #display(events_table.head())\n",
    "\n",
    "# ############# step 3 - Reading KOP data #############\n",
    "# if verbose: print(\"Step 3: Reading and preprocessing Key Opinion Leaders data\")\n",
    "# dir_path_for_KOP_table = os.path.join(KOP_excel_name)\n",
    "# KOP_table = pd.read_excel(dir_path_for_KOP_table)\n",
    "# try:\n",
    "#     KOP_table.rename(columns = {'Unnamed: 0':'KOP_num',\n",
    "#                                 \"Born In\":\"Born_in\",\n",
    "#                                 \"Twitter acount name\" : \"twitter_user_name\",}, inplace = True)\n",
    "#     #drop unnecessary columns\n",
    "#     KOP_table = KOP_table.drop(labels = [\"Unnamed: 7\", \"Source\"], axis = 1)\n",
    "# except: print()\n",
    "\n",
    "# #remove KOP without twitter account name\n",
    "# KOP_table = KOP_table[KOP_table['twitter_user_name'].notna()]\n",
    "# #remove the \"@\" at the begining of some user names\n",
    "# KOP_table[\"clean_user_name\"] = KOP_table[\"twitter_user_name\"].apply(lambda x: x.replace(\"@\", \"\"))\n",
    "\n",
    "# def take_all_except_first_char(x):\n",
    "#     try:\n",
    "#         author_id = tweeterid.handle_to_id(x)\n",
    "#     except:\n",
    "#         author_id = \"-\"\n",
    "#     return author_id\n",
    "\n",
    "# KOP_table[\"author_id\"] = KOP_table[\"clean_user_name\"].apply(take_all_except_first_char)\n",
    "\n",
    "############# step 4-A - Stop-words #############\n",
    "if verbose: print(\"Step 4-A: Reading stop words data and adding your stop words\")\n",
    "stopwords_file_name = os.path.join(stop_words_file_name)\n",
    "stopwords_url = \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\"\n",
    "\n",
    "if not os.path.isfile(stopwords_file_name):\n",
    "    stopwords = requests.get(stopwords_url).text.split()\n",
    "    with open(stopwords_file_name,'w+t', encoding='utf-8') as out_file:\n",
    "        out_file.write(' '.join(stopwords))\n",
    "else:\n",
    "    with open(stopwords_file_name,'rt', encoding='utf-8') as in_file:\n",
    "        stopwords = in_file.readline().split()\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "#### Adding stopWords\n",
    "for word_i in stop_words_to_add:\n",
    "    stopwords.add(word_i)\n",
    "\n",
    "############# step 4-B - contractions #############\n",
    "if verbose: print(\"\\nStep 4-B: Preprocess the tweets table - the text column - using contractions \")\n",
    "def clean_text(x):\n",
    "    from gensim.utils import simple_preprocess\n",
    "    import contractions\n",
    "    try:\n",
    "        x = contractions.fix(x)\n",
    "        x = ' '.join(simple_preprocess(x))\n",
    "    except:\n",
    "        fun = \"fun\"\n",
    "    return x\n",
    "tweets_table['text'] = tweets_table['text'].apply(clean_text)\n",
    "\n",
    "############# step 5 - Preprocess the tweets table for scoring #############\n",
    "if verbose: print(\"\\nStep 5: Preprocess the tweets table for scoring:\\nRemoving Stop words\\\n",
    "| bigram, trigram, forthgram | merging the tweets table with the KOP and Events tables | \")\n",
    "\n",
    "start_time = time.time()\n",
    "#using gensim function to split the text into tokens\n",
    "tweets_table[\"text_tokens\"] = tweets_table[\"text\"].apply(gensim.utils.simple_preprocess,{\"deacc\":True, \"min_len\":2,\"max_len\":25})\n",
    "\n",
    "#remove stopwords:\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "tweets_table['text_tokens'] = tweets_table['text_tokens'].apply(remove_stopwords, stopwords=stopwords)\n",
    "\n",
    "################################## add bygrams to the tokens\n",
    "#print(f\"connector words: {gensim.models.phrases.ENGLISH_CONNECTOR_WORDS}\")\n",
    "## train bygram model\n",
    "bigram = gensim.models.Phrases(tweets_table['text_tokens'], min_count=7, threshold=2, connector_words=gensim.models.phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "if verbose: print(f\"found {len(bigram_model.phrasegrams)} bigrams\")\n",
    "# bigram_model.phrasegrams #the bygrams the model found\n",
    "tweets_table['text_tokens'] = tweets_table['text_tokens'].apply(lambda x: bigram_model[x])\n",
    "###### add trigram to the tokens\n",
    "trigram = gensim.models.Phrases(tweets_table['text_tokens'], min_count=7, threshold=2, connector_words=gensim.models.phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "if verbose: print(f\"found {len(trigram_model.phrasegrams)} trigram\")\n",
    "#trigram_model.phrasegrams #the trigram that the model found\n",
    "tweets_table['text_tokens'] = tweets_table['text_tokens'].apply(lambda x: trigram_model[x])\n",
    "############################################################################\n",
    "###### add forthgram to the tokens\n",
    "forthgram = gensim.models.Phrases(tweets_table['text_tokens'], min_count=7, threshold=6, connector_words=gensim.models.phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "forthgram_model = gensim.models.phrases.Phraser(forthgram)\n",
    "if verbose: print(f\"found {len(forthgram_model.phrasegrams)} forthgram\")\n",
    "#forthgram_model.phrasegrams #the trigram that the model found\n",
    "tweets_table['text_tokens'] = tweets_table['text_tokens'].apply(lambda x: forthgram_model[x])\n",
    "############################################################################\n",
    "### add tweet date column\n",
    "def take_only_10_first_char(x):\n",
    "    return(x[0:10])\n",
    "tweets_table[\"created_at_date\"] = tweets_table[\"created_at\"].apply(take_only_10_first_char)\n",
    "tweets_table[\"created_at_date\"] = pd.to_datetime(tweets_table[\"created_at_date\"])\n",
    "\n",
    "tweets_table[\"Year_tweet\"] = pd.DatetimeIndex(tweets_table['created_at']).year\n",
    "\n",
    "### add column is_in_special_date that checks wheter the tweet was published in a special event day\n",
    "#tweets_table['is_in_special_date'] = tweets_table['created_at_date'].apply(lambda x : pd.Series(x).isin(events_table[\"Date\"]).any())\n",
    "\n",
    "# ### Joining the event table so we can get additional information for the special events\n",
    "# tweets_table = tweets_table.merge(events_table, left_on = \"created_at_date\", right_on = \"Date\", how = \"left\")\n",
    "# tweets_table.rename(columns = {'Date':'Event_Date'}, inplace = True)\n",
    "# tweets_table['Event_Date'] = tweets_table['Event_Date'].fillna(\"No-Event\")\n",
    "\n",
    "# ### Joining the KOP table so we can get additional information for the KOP\n",
    "# tweets_table = tweets_table.merge(KOP_table, left_on = \"author_id\", right_on = \"author_id\", how = \"left\")\n",
    "# try:\n",
    "#     tweets_table['KOP_num'] = tweets_table['KOP_num'].fillna(\"No-KOP\")\n",
    "#     tweets_table['Index'] = tweets_table['Index'].fillna(\"No-KOP\")\n",
    "#     tweets_table['Name'] = tweets_table['Name'].fillna(\"No-KOP\")\n",
    "#     tweets_table['Born_in'] = tweets_table['Born_in'].fillna(\"No-KOP\")\n",
    "#     tweets_table['Role'] = tweets_table['Role'].fillna(\"No-KOP\")\n",
    "#     tweets_table['Place'] = tweets_table['Place'].fillna(\"No-KOP\")\n",
    "# except: print()\n",
    "\n",
    "if verbose: print(\"\\nFinish preprocessing the tweets table, it took:\", round(time.time() - start_time,3), \"seconds\")\n",
    "\n",
    "# ############# step 6 - Scoring #############\n",
    "# if verbose: print(\"Step 6: Scoring the Tweets\")\n",
    "\n",
    "# KOP_ids = list(set(KOP_table.author_id))\n",
    "\n",
    "# ### the following scorer - count the number of times the key words apprear in a certain text. If a certain word (brexit)\n",
    "# # appear more than once, it will be counted twice\n",
    "# def scorer_keywords_brexit(tweet_tokens, key_words):\n",
    "#     #scoring the keywords:\n",
    "#     count_key_words = 0\n",
    "#     for word in key_words:\n",
    "#         count_key_words += tweet_tokens.count(word)\n",
    "#     return count_key_words\n",
    "# ##########################################################################################################\n",
    "# def scorer_KOP_brexit(author_id, KOP_ids):\n",
    "#     if author_id in KOP_ids:\n",
    "#         score = 1\n",
    "#     else:\n",
    "#         score = 0\n",
    "#     return score\n",
    "# ##########################################################################################################\n",
    "# ### adding year column\n",
    "# #tweets_table[\"Year_tweet\"] = pd.DatetimeIndex(tweets_table['created_at']).year\n",
    "\n",
    "# ### scoring key-words\n",
    "# tweets_table[\"score_key_words\"] = tweets_table['text_tokens'].apply(scorer_keywords_brexit, key_words = key_words)\n",
    "\n",
    "# #scoring KOP: if the author is KOP then the score in this column will be 1, else 0\n",
    "# tweets_table[\"score_KOP\"] = tweets_table['author_id'].apply(scorer_KOP_brexit, KOP_ids = KOP_ids)\n",
    "\n",
    "# ### scoring events - column is_in_special_date\n",
    "\n",
    "# #tweets_table[\"total_score\"]: adding the weights of each scorer\n",
    "# tweets_table[\"total_score\"] = score_for_key_words*tweets_table['score_key_words'] + score_for_key_event*tweets_table[\"is_in_special_date\"] + score_for_KOP*tweets_table[\"score_KOP\"]\n",
    "\n",
    "# #Geting a table with all the tweets that passed the score threshold\n",
    "# tweets_table_filtered = tweets_table[tweets_table[\"total_score\"]>=threshold_score].copy()\n",
    "# #Sort the filtered table by score such that the tweets with the highest score will be first\n",
    "# tweets_table_filtered.sort_values(by = \"total_score\", ascending=False, inplace=True)\n",
    "\n",
    "print(\"\\nFinish!\\nTotal time:\", round(time.time() - start_fun_time,3),\n",
    "    \"Seconds (\",round((time.time() - start_fun_time)/60,3), \"Minutes)\")\n",
    "\n",
    "bigrams_models = [bigram, trigram, forthgram]\n",
    "#return (tweets_table, csv_files_evaluated, bigrams_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bebde64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T13:30:08.854627Z",
     "start_time": "2022-06-17T13:30:08.824646Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets=979821\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of tweets={tweets_table.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391c896",
   "metadata": {},
   "source": [
    "## Writing the filtered table to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_table.to_csv(os.path.join(data_folder_name, \"tweets_table_all.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205506c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050a6fc2",
   "metadata": {},
   "source": [
    "# Not relevant:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919413a8",
   "metadata": {},
   "source": [
    "### Add basic sentiment using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a9e1c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T13:34:40.101289Z",
     "start_time": "2022-06-17T13:34:32.124817Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Roy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def nltk_sentiment(row, sentiment_intensity_analyzer, column_text_name = \"text\"):\n",
    "    import numpy as np, pandas as pd\n",
    "    sentiment_names = ['negative', 'neutral', 'positive']\n",
    "    nltk_sentiment = sentiment_intensity_analyzer.polarity_scores(row[column_text_name])\n",
    "    \n",
    "    #nltk_sentiment['nltk_verdict'] = sentiment_names[np.argmax([nltk_sentiment[s] for s in ['neg','neu','pos']])]\n",
    "    if nltk_sentiment['neu']>0.95:\n",
    "        nltk_sentiment['nltk_verdict'] = 'neutral'\n",
    "    elif nltk_sentiment['neg'] < nltk_sentiment['pos']:\n",
    "        nltk_sentiment['nltk_verdict'] = 'positive'\n",
    "    else:\n",
    "        nltk_sentiment['nltk_verdict'] = 'negative'\n",
    "    #nltk_sentiment['ground_truth'] = row['airline_sentiment']\n",
    "    #nltk_sentiment['ground_truth_value'] = {'negative': -1, 'neutral':0, 'positive':1 }[nltk_sentiment['ground_truth']]\n",
    "    return pd.Series(nltk_sentiment)\n",
    "\n",
    "\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sentiment_intensity_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "nltk_sentiment_tweets_table = tweets_table.apply(nltk_sentiment, sentiment_intensity_analyzer=sentiment_intensity_analyzer, column_text_name = \"text\"  ,axis=1)\n",
    "\n",
    "##### Adding the sentiment analysis columns\n",
    "tweets_table[\"KOL_Tweet_negative\"] = nltk_sentiment_tweets_table.neg\n",
    "tweets_table[\"KOL_Tweet_neutural\"] = nltk_sentiment_tweets_table.neu\n",
    "tweets_table[\"KOL_Tweet_positive\"] = nltk_sentiment_tweets_table.pos\n",
    "tweets_table[\"KOL_Tweet_compound\"] = nltk_sentiment_tweets_table.compound\n",
    "\n",
    "#### This columns contain the final \"verdict\" of the NLTK Model\n",
    "tweets_table[\"KOL_Tweet_nltk_verdict\"] = nltk_sentiment_tweets_table.nltk_verdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c08f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T13:59:07.002679Z",
     "start_time": "2022-06-17T13:59:07.002679Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000264da",
   "metadata": {},
   "source": [
    "### Writing the filtered table to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c3bcb17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T11:18:29.251059Z",
     "start_time": "2022-06-16T11:06:48.817659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_folder'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets_table.to_excel(os.path.join(data_folder_name, \"tweets_table_all3.xlsx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300e1679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T13:33:03.058006Z",
     "start_time": "2022-06-17T13:30:51.614008Z"
    }
   },
   "outputs": [],
   "source": [
    "### write to csv\n",
    "tweets_table.to_csv(os.path.join(data_folder_name, \"tweets_table_all.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ce9c613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T13:01:15.714345Z",
     "start_time": "2022-06-15T13:01:12.272462Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweets_table_filtered.to_excel(os.path.join(data_folder_name, \"tweets_table_filtered2.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32213021",
   "metadata": {},
   "source": [
    "## Add language classification\n",
    "And filter the non English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "language_classifications = []\n",
    "for row in tqdm_notebook(tweets_table.text): #tqdm_notebook - to pring progress bar\n",
    "    language_classification, score = langid.classify(row)\n",
    "    language_classifications.append(language_classification)\n",
    "\n",
    "\n",
    "tweets_table[\"language\"] = language_classifications\n",
    "tweets_table = tweets_table[tweets_table[\"language\"] == \"en\"]\n",
    "\n",
    "### reset index\n",
    "tweets_table.reset_index(inplace=True)\n",
    "tweets_table[\"index\"] = tweets_table.index\n",
    "\n",
    "print(f\"Total number of tweets of the KOL in English={tweets_table.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a742649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T15:40:28.294556Z",
     "start_time": "2022-06-15T15:40:27.924762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>author_id_new</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>users.name</th>\n",
       "      <th>Year_tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jeremy Vine</th>\n",
       "      <th>2019</th>\n",
       "      <td>19406</td>\n",
       "      <td>19406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChukaUmunna</th>\n",
       "      <th>2019</th>\n",
       "      <td>8512</td>\n",
       "      <td>8512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sadiq Khan</th>\n",
       "      <th>2019</th>\n",
       "      <td>7318</td>\n",
       "      <td>7318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Jeremy Vine</th>\n",
       "      <th>2018</th>\n",
       "      <td>7171</td>\n",
       "      <td>7171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>7118</td>\n",
       "      <td>7118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rupert Murdoch</th>\n",
       "      <th>2017</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rt. Hon. Michael Heseltine</th>\n",
       "      <th>2015</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Paul Dacre</th>\n",
       "      <th>2016</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olly Robbins</th>\n",
       "      <th>2018</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       author_id_new     id\n",
       "users.name                 Year_tweet                      \n",
       "Jeremy Vine                2019                19406  19406\n",
       "ChukaUmunna                2019                 8512   8512\n",
       "Sadiq Khan                 2019                 7318   7318\n",
       "Jeremy Vine                2018                 7171   7171\n",
       "                           2016                 7118   7118\n",
       "...                                              ...    ...\n",
       "Rupert Murdoch             2017                    1      1\n",
       "Rt. Hon. Michael Heseltine 2015                    1      1\n",
       "Paul Dacre                 2016                    1      1\n",
       "                           2014                    1      1\n",
       "Olly Robbins               2018                    1      1\n",
       "\n",
       "[326 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the number of tweets per KOL\n",
    "tweets_table[[\"author_id_new\", \"id\", \"users.name\", \"Year_tweet\"]].groupby(by = [\"users.name\", \"Year_tweet\"]).count().sort_values(by = \"id\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49073323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T15:41:13.026964Z",
     "start_time": "2022-06-15T15:41:12.969004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>author_id_new</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>users.name</th>\n",
       "      <th>Year_tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Charles Grant</th>\n",
       "      <th>2017</th>\n",
       "      <td>869</td>\n",
       "      <td>869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>852</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Martin Selmayr</th>\n",
       "      <th>2017</th>\n",
       "      <td>776</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChukaUmunna</th>\n",
       "      <th>2017</th>\n",
       "      <td>709</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charles Grant</th>\n",
       "      <th>2019</th>\n",
       "      <td>684</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr Liam Fox MP</th>\n",
       "      <th>2014</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steve O'Connell</th>\n",
       "      <th>2014</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dyson</th>\n",
       "      <th>2018</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matteo Renzi</th>\n",
       "      <th>2014</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rupert Murdoch</th>\n",
       "      <th>2017</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            author_id_new   id\n",
       "users.name      Year_tweet                    \n",
       "Charles Grant   2017                  869  869\n",
       "                2016                  852  852\n",
       "Martin Selmayr  2017                  776  776\n",
       "ChukaUmunna     2017                  709  709\n",
       "Charles Grant   2019                  684  684\n",
       "...                                   ...  ...\n",
       "Dr Liam Fox MP  2014                    1    1\n",
       "Steve O'Connell 2014                    1    1\n",
       "Dyson           2018                    1    1\n",
       "Matteo Renzi    2014                    1    1\n",
       "Rupert Murdoch  2017                    1    1\n",
       "\n",
       "[294 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the number of tweets per KOL\n",
    "tweets_table_filtered[[\"author_id_new\", \"id\", \"users.name\", \"Year_tweet\"]].groupby(by = [\"users.name\", \"Year_tweet\"]).count().sort_values(by = \"id\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29aeaabf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-16T11:46:54.075896Z",
     "start_time": "2022-06-16T11:46:53.407212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id_new</th>\n",
       "      <th>id</th>\n",
       "      <th>users.name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>127945</td>\n",
       "      <td>127945</td>\n",
       "      <td>127945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>68602</td>\n",
       "      <td>68602</td>\n",
       "      <td>68602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>67645</td>\n",
       "      <td>67645</td>\n",
       "      <td>67645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>61387</td>\n",
       "      <td>61387</td>\n",
       "      <td>61387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>56626</td>\n",
       "      <td>56626</td>\n",
       "      <td>56626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>45455</td>\n",
       "      <td>45455</td>\n",
       "      <td>45455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>8090</td>\n",
       "      <td>8090</td>\n",
       "      <td>8090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id_new      id  users.name\n",
       "Year_tweet                                   \n",
       "2019               127945  127945      127945\n",
       "2017                68602   68602       68602\n",
       "2016                67645   67645       67645\n",
       "2018                61387   61387       61387\n",
       "2015                56626   56626       56626\n",
       "2014                45455   45455       45455\n",
       "2020                 8090    8090        8090"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_table[[\"author_id_new\", \"id\", \"users.name\", \"Year_tweet\"]].groupby(by = [\"Year_tweet\"]).count().sort_values(by = \"id\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f1811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
