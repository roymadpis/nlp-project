{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51bd41b4",
   "metadata": {},
   "source": [
    "# Step_6_Visualizing_Word2Vec and Sentiment Analysis\n",
    "### Roy Madpis (319091526) And Michael Kobaivanov (206814485)\n",
    "\n",
    "In this notebook we use all the data we collected so far to collect insights regarding the Brexit issue.\n",
    "    \n",
    "### 1. Sentiment Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c688cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T06:12:13.332063Z",
     "start_time": "2022-06-18T06:12:00.407029Z"
    }
   },
   "outputs": [],
   "source": [
    "#! pip3 freeze > roy_libs1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92e6eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T16:38:44.208960Z",
     "start_time": "2022-06-18T16:38:40.687920Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "#! pip install --trusted-host=pypi.org --trusted-host=files.pythonhosted.org --user gensim\n",
    "#! pip install dask\n",
    "#! pip install tabulate\n",
    "#! pip install nltk\n",
    "#! pip install langid # for language detection\n",
    "# ! pip install contractions #for cleaning the text - converting abbriviations to long text for example\n",
    "\n",
    "### downloading the 4.5M tweets with sentiment\n",
    "#! curl --remote-name-all https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1135{/Brexit_tweets_stance.zip}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f9082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.022524Z",
     "start_time": "2022-06-24T11:31:24.022524Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os\n",
    "import seaborn as sns\n",
    "\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import psutil\n",
    "import sys, requests\n",
    "import time, datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dask import dataframe as dd\n",
    "import collections\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "#for readin zip files\n",
    "import zipfile\n",
    "import bz2\n",
    "import itertools\n",
    "import codecs\n",
    "import io\n",
    "\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gensim\n",
    "import inspect\n",
    "import contractions\n",
    "import langid #for language detection\n",
    "#import logging\n",
    "#logging.basicConfig(format ='%(asctime)s: %(levelname)s: %(message)s', level = logging.ERROR)\n",
    "\n",
    "#stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "#####################################\n",
    "data_folder = \"data_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1c2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.025925Z",
     "start_time": "2022-06-24T11:31:24.025925Z"
    }
   },
   "outputs": [],
   "source": [
    "### reading the stopwords file / downloading it and add some more stopwords\n",
    "\n",
    "stopwords_file_name =  os.path.join(data_folder, \"StopWords\")\n",
    "stopwords_url = \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\"\n",
    "\n",
    "\n",
    "stop_words_to_add = [\"http\", \"https\", \"rt\", \"co\", \"vrkhaxde\", \"oeblog\", \"rln\", \"simonjhix\", \"cctvqfr\",\n",
    "\"dcpj\", \"xvy\", \"mycekvwlxr\", \"imydbvvwji\", \"kkd\", \"rwp\",\"yfc\",\"fus\",\"tmawgoafhb\",\"edzmidvpel\", \"brexit.\", \"brexit,\",\n",
    "\"xmljwysih\", \"lnfgc\", \"https//t.co/gx1lpjz5ux\", \"gvabanwvhi\", \"fmgynqxrs\", \"https//t.co/zpwcyltwgk\", \"https//t.co/eacccme91i\",\n",
    "\"https//t.co/memxwp57du\", \"https//t.co/kgjzyab6vu\", \"vxpgogryp\", \"https//t.co/l7cfc70l1w\"] ### add more stopwords you want\n",
    "#################################################################\n",
    "if not os.path.isfile(stopwords_file_name):\n",
    "    StopWords = requests.get(stopwords_url).text.split()\n",
    "    with open(stopwords_file_name,'w+t', encoding='utf-8') as out_file:\n",
    "        out_file.write(' '.join(StopWords))\n",
    "else:\n",
    "    with open(stopwords_file_name,'rt', encoding='utf-8') as in_file:\n",
    "        StopWords = in_file.readline().split()\n",
    "StopWords = set(StopWords)\n",
    "\n",
    "for word_i in stop_words_to_add:\n",
    "    StopWords.add(word_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdd298",
   "metadata": {},
   "source": [
    "## 5.1.a Reading Key Opinion leaders tweets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6daf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T11:45:07.556334Z",
     "start_time": "2022-05-13T11:45:07.543193Z"
    }
   },
   "source": [
    "a. We split the ~1M tweets data (data_folder/tweets_table_all_with_sentimet.csv) by the year the tweet was published, train a Word2Vec model for each year alone (thus have word2vec model that was trained on tweets from 2014 / model that was trained on tweets from 2015, etc) we also have one Word2Vec model that was trained on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d09cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.031027Z",
     "start_time": "2022-06-24T11:31:24.031027Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_with_all_tweets = os.path.join(data_folder,\"Predicted_sentiment_tables\")\n",
    "\n",
    "print(\"Step 1: Reading all the csv files\")\n",
    "tweets_tables = []\n",
    "csv_files_evaluated = []\n",
    "for root,dirs,files in os.walk(dir_with_all_tweets):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"): #if the file is csv\n",
    "            csv_files_evaluated.append(file)\n",
    "            file_location = os.path.join(dir_with_all_tweets, file)\n",
    "            tweets_tables.append(pd.read_csv(file_location))\n",
    "\n",
    "tweets_table_all_with_sentimet_0 = pd.concat(tweets_tables)\n",
    "print(\"There are:\", tweets_table_all_with_sentimet_0.shape[0], \"Tweets\")\n",
    "\n",
    "tweets_table_all_with_sentimet_0 = tweets_table_all_with_sentimet_0.drop(labels = [\"Unnamed: 0\", \"index_num\"], axis = 1)\n",
    "tweets_table_all_with_sentimet_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eecd615",
   "metadata": {},
   "source": [
    "### Reading the table with 300k tweets with sentiments\n",
    "This is the data that we read from twitter using th tweet-ids) [this data is out of the corpus of 4.5M tweet-ids with sentiment]\n",
    "\n",
    "We need to preprocess that table so it would be in the same format and shape as the table we have above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e2837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.036427Z",
     "start_time": "2022-06-24T11:31:24.036427Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder_name_for_sentiment = \"data_folder/tweets_4_5M/tweet_ids_with_sentiment\"\n",
    "\n",
    "print(\"Step 1: Reading all the csv files\")\n",
    "tweets_tables = []\n",
    "csv_files_evaluated = []\n",
    "for root,dirs,files in os.walk(data_folder_name_for_sentiment):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"): #if the file is csv\n",
    "            csv_files_evaluated.append(file)\n",
    "            file_location = os.path.join(data_folder_name_for_sentiment, file)\n",
    "            tweets_tables.append(pd.read_csv(file_location))\n",
    "\n",
    "tweets_with_sentiment_300k = pd.concat(tweets_tables)\n",
    "print(\"There are:\", tweets_with_sentiment_300k.shape[0], \"Tweets\")\n",
    "\n",
    "### preprocess\n",
    "\n",
    "## change the sentiment column name\n",
    "tweets_with_sentiment_300k = tweets_with_sentiment_300k.rename({\"sentimnt\":\"Predicted_sentiment\"}, axis = 1)\n",
    "\n",
    "tweets_with_sentiment_300k[\"text_tokens\"] = 0\n",
    "\n",
    "def take_only_10_first_char(x):\n",
    "    return(x[0:10])\n",
    "tweets_with_sentiment_300k[\"created_at_date\"] = tweets_with_sentiment_300k[\"created_at\"].apply(take_only_10_first_char)\n",
    "tweets_with_sentiment_300k[\"created_at_date\"] = pd.to_datetime(tweets_with_sentiment_300k[\"created_at_date\"])\n",
    "\n",
    "tweets_with_sentiment_300k[\"Year_tweet\"] = pd.DatetimeIndex(tweets_with_sentiment_300k['created_at']).year\n",
    "tweets_with_sentiment_300k = tweets_with_sentiment_300k.drop(labels = [\"Unnamed: 0\",\"tweet_id\"], axis = 1)\n",
    "\n",
    "tweets_with_sentiment_300k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4dfb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.040948Z",
     "start_time": "2022-06-24T11:31:24.040948Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tweets_table_all_with_sentimet_0.shape)\n",
    "print(tweets_with_sentiment_300k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293176b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.045954Z",
     "start_time": "2022-06-24T11:31:24.045453Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### combining the two tables into one big table\n",
    "tweets_table_all_with_sentimet = pd.concat([tweets_table_all_with_sentimet_0, tweets_with_sentiment_300k])\n",
    "print(\"Total number of tweets with sentiment =\", tweets_table_all_with_sentimet.shape[0])\n",
    "\n",
    "### remove duplicates\n",
    "tweets_table_all_with_sentimet = tweets_table_all_with_sentimet.drop_duplicates(subset=['author_id_new', 'conv_id_new', \"id_new\", \"created_at\", \"public_metrics.like_count\", \"text\"])\n",
    "print(\"Total number of tweets with sentiment not duplicate =\", tweets_table_all_with_sentimet.shape[0])\n",
    "\n",
    "tweets_table_all_with_sentimet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "964d0922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-19T09:37:20.372677Z",
     "start_time": "2022-06-19T09:37:20.346645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number or rows in each group: 9.8\n"
     ]
    }
   ],
   "source": [
    "# num_of_rows = tweets_table_all_with_sentimet.shape[0]\n",
    "# num_groups = 100\n",
    "# print(\"Number or rows in each group:\", round(num_of_rows/num_groups,1))\n",
    "# split_groups = np.array_split(range(num_of_rows), num_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651de2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T13:16:57.560037Z",
     "start_time": "2022-05-27T13:16:57.518064Z"
    }
   },
   "source": [
    "### Preprocess the filtered tweets\n",
    "After we read the tweets filtered table, we need to perform several cleaning / preprocessing on those filtered tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc8fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.050590Z",
     "start_time": "2022-06-24T11:31:24.050590Z"
    }
   },
   "outputs": [],
   "source": [
    "# def clean_text(x):\n",
    "#     from gensim.utils import simple_preprocess\n",
    "#     import contractions\n",
    "#     x = contractions.fix(x)\n",
    "#     x = ' '.join(simple_preprocess(x))\n",
    "#     return x\n",
    "\n",
    "### Pre-processing columns conv_id_new and author_id_new\n",
    "tweets_table_all_with_sentimet['conversation_id'] = tweets_table_all_with_sentimet['conv_id_new'].apply(lambda x: x.replace(\"conv_id: \", \"\"))\n",
    "tweets_table_all_with_sentimet['author_id'] = tweets_table_all_with_sentimet['author_id_new'].apply(lambda x: x.replace(\"author_id: \", \"\"))\n",
    "#tweets_table_all_with_sentimet['clean_text'] = tweets_table_all_with_sentimet['text'].apply(clean_text)\n",
    "tweets_table_all_with_sentimet = tweets_table_all_with_sentimet[tweets_table_all_with_sentimet['text'].notna()] #remove rows with nan values in the text column\n",
    "\n",
    "tweets_table_all_with_sentimet['clean_text'] = tweets_table_all_with_sentimet['text']\n",
    "\n",
    "if \"Year_tweet\" in tweets_table_all_with_sentimet.columns:\n",
    "    years_tweets = tweets_table_all_with_sentimet.Year_tweet\n",
    "else:\n",
    "    years_tweets = pd.DatetimeIndex(tweets_table_all_with_sentimet['created_at_date']).year\n",
    "    tweets_table_all_with_sentimet[\"Year_tweet\"] = years_tweets\n",
    "### detecting the language of the text + preprocess it\n",
    "language_classifications = []\n",
    "tokens_roy = []\n",
    "years_tweets = []\n",
    "for row in tqdm_notebook(tweets_table_all_with_sentimet.clean_text): #tqdm_notebook - to pring progress bar\n",
    "    #language_classification, score = langid.classify(row)\n",
    "    #language_classifications.append(language_classification)\n",
    "\n",
    "    data_clean = row.lower().replace(\"#\", \"\").replace(\"@\", \"\").replace(\"?\", \"\").replace(\"\\\"\", \"\").replace(\"\\'\", \"\").replace(\":\", \"\")\n",
    "    data_clean = data_clean.split()\n",
    "    data_clean = [token for token in data_clean if token not in StopWords]\n",
    "    data_clean = [token for token in data_clean if len(token) >=2]\n",
    "    tokens_roy.append(data_clean) #\n",
    "\n",
    "texts = tweets_table_all_with_sentimet.text\n",
    "text_tokens = tweets_table_all_with_sentimet.text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef432bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T07:14:45.115115Z",
     "start_time": "2022-06-17T07:14:45.111113Z"
    }
   },
   "outputs": [],
   "source": [
    "# if \"language\" in filtered_table.columns:\n",
    "#     language_classifications.append(language_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a12936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.054437Z",
     "start_time": "2022-06-24T11:31:24.054437Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(language_classifications))\n",
    "print(len(tokens_roy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538c4ec",
   "metadata": {},
   "source": [
    "### splitting the tweets to the relevant years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa82ef3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:44:21.518040Z",
     "start_time": "2022-06-24T09:44:21.505203Z"
    }
   },
   "outputs": [],
   "source": [
    "#years_tweets = pd.DatetimeIndex(tweets_table_all_with_sentimet['created_at_date']).year\n",
    "years_tweets = tweets_table_all_with_sentimet['Year_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd7a2c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:44:27.125885Z",
     "start_time": "2022-06-24T09:44:27.066349Z"
    }
   },
   "outputs": [],
   "source": [
    "years_tweets_2014 = [years_tweets==2014]\n",
    "years_tweets_2014 = np.asarray(years_tweets_2014).reshape(-1,1)\n",
    "\n",
    "years_tweets_2015 = [years_tweets==2015]\n",
    "years_tweets_2015 = np.asarray(years_tweets_2015).reshape(-1,1)\n",
    "\n",
    "years_tweets_2016 = [years_tweets==2016]\n",
    "years_tweets_2016 = np.asarray(years_tweets_2016).reshape(-1,1)\n",
    "\n",
    "years_tweets_2017 = [years_tweets==2017]\n",
    "years_tweets_2017 = np.asarray(years_tweets_2017).reshape(-1,1)\n",
    "\n",
    "years_tweets_2018 = [years_tweets==2018]\n",
    "years_tweets_2018 = np.asarray(years_tweets_2018).reshape(-1,1)\n",
    "\n",
    "years_tweets_2019 = [years_tweets==2019]\n",
    "years_tweets_2019 = np.asarray(years_tweets_2019).reshape(-1,1)\n",
    "\n",
    "years_tweets_2020 = [years_tweets==2020]\n",
    "years_tweets_2020 = np.asarray(years_tweets_2020).reshape(-1,1)\n",
    "\n",
    "years_tweets_2021 = [years_tweets==2021]\n",
    "years_tweets_2021 = np.asarray(years_tweets_2021).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1796052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-23T13:14:20.764027Z",
     "start_time": "2022-06-23T13:14:15.734860Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_df = pd.DataFrame(tokens_roy)#[years_tweets_2018]\n",
    "tokens_df_2014 = tokens_df[years_tweets_2014]\n",
    "tokens_df_2015 = tokens_df[years_tweets_2015]\n",
    "tokens_df_2016 = tokens_df[years_tweets_2016]\n",
    "tokens_df_2017 = tokens_df[years_tweets_2017]\n",
    "tokens_df_2018 = tokens_df[years_tweets_2018]\n",
    "tokens_df_2019 = tokens_df[years_tweets_2019]\n",
    "tokens_df_2020 = tokens_df[years_tweets_2020]\n",
    "tokens_df_2021 = tokens_df[years_tweets_2021]\n",
    "\n",
    "tokens_df_all = tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d4253a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:44:33.218839Z",
     "start_time": "2022-06-24T09:44:32.129546Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_df_2014' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25480\\1161977059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2014:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_df_2014\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2015:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_df_2015\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2016:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_df_2016\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2017:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_df_2017\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2018:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_df_2018\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_df_2014' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"2014:\", tokens_df_2014.shape)\n",
    "print(\"2015:\", tokens_df_2015.shape)\n",
    "print(\"2016:\", tokens_df_2016.shape)\n",
    "print(\"2017:\", tokens_df_2017.shape)\n",
    "print(\"2018:\", tokens_df_2018.shape)\n",
    "print(\"2019:\", tokens_df_2019.shape)\n",
    "print(\"2020:\", tokens_df_2020.shape)\n",
    "print(\"2021:\", tokens_df_2020.shape)\n",
    "print(\"All:\", tokens_df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ff744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:44:33.221478Z",
     "start_time": "2022-06-24T09:44:33.221478Z"
    }
   },
   "outputs": [],
   "source": [
    "### this function takes the data frame and takes all the None values out + transform it to list of list (each row becomes a list)\n",
    "def tokens_df_new(tokens_df):\n",
    "    tokens_df_new = []\n",
    "    for list_i in tokens_df.values.tolist():\n",
    "        list_i_new = []\n",
    "        for i in list_i:\n",
    "            if i != None:\n",
    "                list_i_new.append(i)\n",
    "\n",
    "        tokens_df_new.append(list_i_new)\n",
    "    return(tokens_df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a672052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:44:33.223762Z",
     "start_time": "2022-06-24T09:44:33.223762Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_df_new_2014 = tokens_df_new(tokens_df_2014)\n",
    "tokens_df_new_2015 = tokens_df_new(tokens_df_2015)\n",
    "tokens_df_new_2016 = tokens_df_new(tokens_df_2016)\n",
    "tokens_df_new_2017 = tokens_df_new(tokens_df_2017)\n",
    "tokens_df_new_2018 = tokens_df_new(tokens_df_2018)\n",
    "tokens_df_new_2019 = tokens_df_new(tokens_df_2019)\n",
    "tokens_df_new_2020 = tokens_df_new(tokens_df_2020)\n",
    "tokens_df_new_2021 = tokens_df_new(tokens_df_2021)\n",
    "tokens_df_all = tokens_df_new(tokens_df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c477c342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:24.016607Z",
     "start_time": "2022-06-24T11:31:23.927102Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_df_new_2014' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25480\\2963584379.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_df_new_2014\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_df_new_2014' is not defined"
     ]
    }
   ],
   "source": [
    "len(tokens_df_new_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aaaae7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:31:05.906893Z",
     "start_time": "2022-06-24T11:31:05.870265Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757371ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3f069eb",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32096eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "509e7108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:34:02.841560Z",
     "start_time": "2022-06-24T11:34:02.821986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        rt downingstreet more than countries places in...\n",
       "1        rt gavinwilliamson want to thank all those ser...\n",
       "2        rt ukgovwales welsh secretary aluncairns exten...\n",
       "3                          tickettattle tfwrail sure thing\n",
       "4        wpl_official barrytownunited the_nomads balato...\n",
       "                               ...                        \n",
       "62542    RT @RichardWellings: Good the Brexit debate is...\n",
       "62543    RT @BelTel: EU Referendum: Northern Ireland sa...\n",
       "62544    RT @K8teMorgan: Regardless of your voting pref...\n",
       "62545    RT @Vote_LeaveMedia: PM's former strategy chie...\n",
       "62546    RT @Keroppo: One of the main donors of the Rem...\n",
       "Name: clean_text, Length: 1285805, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_table_all_with_sentimet.clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c6ddd46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:33:48.572308Z",
     "start_time": "2022-06-24T11:33:48.564247Z"
    }
   },
   "outputs": [],
   "source": [
    "#wordcloud install from anaconda prompt:\n",
    "#conda install -c conda-forge wordcloud \n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212372b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32895a8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:34:59.381695Z",
     "start_time": "2022-06-24T11:34:48.162534Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "tweets_texts = list(map(lambda x: x.lower(), tweets_table_all_with_sentimet.clean_text)) #lower_case\n",
    "#remove punctuation and save in one big string (so we can split all the corpus into words)\n",
    "tweets_texts = str(tweets_texts).translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63548563",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-24T11:35:02.214Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16004d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wordcloud - Positive tweets\")\n",
    "wordcloud_positive = WordCloud(max_font_size=40)\n",
    "wordcloud_positive = wordcloud_positive.generate(positive_tweets)\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "plt.imshow(wordcloud_positive, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show();\n",
    "fig.savefig('local_data/486.full.png', transparent=True, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425b6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402d114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3515314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-27T16:19:12.082121Z",
     "start_time": "2022-05-27T16:19:12.075122Z"
    }
   },
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Here we present the EDA we conducted using the sentiment prediction (that was the outcome of the sentiment model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c7745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0ceab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
